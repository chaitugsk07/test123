{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import feedparser\n",
    "import re\n",
    "import numpy\n",
    "from datetime import datetime, timezone\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from keybert.llm import TextGeneration\n",
    "from keybert import KeyLLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import spacy\n",
    "from keybert import KeyBERT\n",
    "import torch\n",
    "\n",
    "endpoint = \"https://active-monitor-48.hasura.app/v1/graphql\"\n",
    "admin_key = \"bAQuK7HSYvMAp6S6pnqXH0wQlyuKNUICzoW3jwecc27pwz6COLhE750s5YAec7Hz\"\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "gpu = spacy.prefer_gpu()\n",
    "print(gpu)\n",
    "\n",
    "def query_hasura_graphql(endpoint, admin_key, query, variables):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'x-hasura-admin-secret': f'{admin_key}'\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'query': query,\n",
    "        'variables': variables\n",
    "    }\n",
    "    response = requests.post(endpoint, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def is_valid_timezone_format(published):\n",
    "    try:\n",
    "        # Attempt to parse the string\n",
    "        date_format = \"%a, %d %b %Y %H:%M:%S %z\"\n",
    "        date_object = datetime.strptime(published, date_format)\n",
    "\n",
    "        hasura_timestamp = date_object.astimezone(timezone.utc).isoformat()\n",
    "        return True, hasura_timestamp\n",
    "    except ValueError:\n",
    "        # If parsing fails, the string is not in the correct format\n",
    "        return False, None\n",
    "\n",
    "def check_date_format(date_string):\n",
    "    try:\n",
    "        datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%S%z')\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def mutation_hasura_graphql(endpoint, admin_key, mutation_query, mutation_variables):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'x-hasura-admin-secret': f'{admin_key}'\n",
    "    }\n",
    "    response = requests.post(endpoint, json={'query': mutation_query, 'variables': mutation_variables}, headers=headers)\n",
    "    if response.ok:\n",
    "        data = response.json()\n",
    "        print(data)\n",
    "        return True, data\n",
    "    else:\n",
    "        print(f\"Mutation failed with status code {response.status_code}: {response.text}\")\n",
    "        return False, None\n",
    "\n",
    "def update_articles():\n",
    "    graphql_query = '''\n",
    "    query MyQuery($link_type: Int!) {\n",
    "        synopse_articles_t_v1_rss1_feed_links(where: {rss1_link_type: {_eq: $link_type}}) {\n",
    "          rss1_link\n",
    "          rss1_link_name\n",
    "          outlet\n",
    "        }\n",
    "      }\n",
    "    '''\n",
    "    # Define the variables dictionary\n",
    "    variables = {\n",
    "        \"link_type\": 11\n",
    "    }\n",
    "    rss1_links_array = []\n",
    "    rss1_link_name = []\n",
    "    outlet = []\n",
    "    response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "    if response_data:\n",
    "        rss1_links_array = [item[\"rss1_link\"] for item in response_data[\"data\"][\"synopse_articles_t_v1_rss1_feed_links\"]]\n",
    "        outlet = [item[\"outlet\"] for item in response_data[\"data\"][\"synopse_articles_t_v1_rss1_feed_links\"]]\n",
    "        rss1_link_name = [item[\"rss1_link_name\"] for item in response_data[\"data\"][\"synopse_articles_t_v1_rss1_feed_links\"]]\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [synopse_articles_t_v1_rss1_articles_insert_input!] = {}) {\n",
    "        insert_synopse_articles_t_v1_rss1_articles(objects: $objects, on_conflict: {constraint: t_v1_rss1_articals_post_link_key}) {\n",
    "            affected_rows\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    for i in range(0,len(rss1_links_array)):\n",
    "        NewsFeed = feedparser.parse(rss1_links_array[i])\n",
    "        print(\"############################################################\")\n",
    "        print(rss1_links_array[i])\n",
    "        articles = []\n",
    "        for entry in NewsFeed.entries:\n",
    "            if outlet[i] +\".\" in entry.link:\n",
    "              is_default_image = 0\n",
    "              title = entry.title\n",
    "              summary = ''\n",
    "              if 'summary' in entry:\n",
    "                  summary_nofil = entry.summary\n",
    "                  summary = re.sub('<[^<]+?>', '', summary_nofil)\n",
    "              image_url = \"\"\n",
    "              if 'media_content' in entry:\n",
    "                  image_url = entry['media_content'][0]['url']\n",
    "                  is_default_image = 1\n",
    "              if 'links' in entry:\n",
    "                  for link in entry.links:\n",
    "                      if link.type == \"image/jpeg\":\n",
    "                          image_url= link.href\n",
    "                          is_default_image = 1\n",
    "                          break\n",
    "              post_link = entry.link\n",
    "              published = datetime.now(timezone.utc).isoformat()\n",
    "              if 'published' in entry:\n",
    "                  published = entry.published\n",
    "              datevalidation = is_valid_timezone_format(published)\n",
    "              if datevalidation[0]:\n",
    "                  hasura_timestamp = datevalidation[1]\n",
    "              if check_date_format(published):\n",
    "                  hasura_timestamp = published\n",
    "              else:\n",
    "                  hasura_timestamp = datetime.now().astimezone(timezone.utc).isoformat()\n",
    "              if \"author\" in entry:\n",
    "                  author = entry.author\n",
    "              else:\n",
    "                  author = \"na\"\n",
    "              tags = []\n",
    "              tags.append(rss1_link_name[i])\n",
    "              tags.append(outlet[i])\n",
    "              if 'tags' in entry:\n",
    "                  for tag in entry.tags:\n",
    "                    tags.append(tag.term)\n",
    "              if outlet[i] in post_link:\n",
    "                  articles.append({\n",
    "                          \"rss1_link\": rss1_links_array[i],\n",
    "                          \"post_link\": post_link,\n",
    "                          \"title\": title,\n",
    "                          \"summary\": summary,\n",
    "                          \"author\": author,\n",
    "                          \"image_link\" : image_url,\n",
    "                          \"post_published\": hasura_timestamp,\n",
    "                          \"is_default_image\": is_default_image,\n",
    "                          \"tags\": tags,\n",
    "                      }\n",
    "                  )\n",
    "            #print(feed_link, post_link, title, summary, author, image_url, hasura_timestamp, is_default_image)\n",
    "        mutation_variables = {\n",
    "            \"objects\": articles\n",
    "        }\n",
    "        #print({'query': mutation_query, 'variables': mutation_variables})\n",
    "        out1 = mutation_hasura_graphql(endpoint = endpoint, admin_key = admin_key, mutation_query = mutation_query, mutation_variables = mutation_variables)\n",
    "\n",
    "def summerizer(offset1):\n",
    "  # Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "      model_file=\"mistral-7b-instruct-v0.1.Q8_0.gguf\",\n",
    "      model_type=\"mistral\",\n",
    "      gpu_layers=110,\n",
    "      hf=True,\n",
    "      context_length=4000\n",
    "  )\n",
    "  # Tokenizer\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "  # Pipeline\n",
    "  generator = pipeline(\n",
    "      model=model,\n",
    "      tokenizer=tokenizer,\n",
    "      task='text-generation',\n",
    "      max_new_tokens=2000,\n",
    "      min_new_tokens=200,\n",
    "      repetition_penalty=1.1\n",
    "  )\n",
    "  graphql_query = '''\n",
    "  query MyQuery($offset: Int = 0, $limit: Int = 10) {\n",
    "    synopse_articles_t_v1_rss1_articles(offset: $offset, where: {is_in_detail: {_eq: 1}, is_summerized: {_eq: 0}}, order_by: {created_at: desc}, limit: $limit) {\n",
    "      title\n",
    "      summary\n",
    "      t_v1_rss1_articles_detail {\n",
    "        description\n",
    "      }\n",
    "      id\n",
    "    }\n",
    "  }\n",
    "  '''\n",
    "  offset = offset1\n",
    "  mutation_query = \"\"\"\n",
    "  mutation MyMutation($objects: [synopse_articles_t_v2_articles_summary_insert_input!] = {}, $updates: [synopse_articles_t_v1_rss1_articles_updates!] = {where: {}}) {\n",
    "    insert_synopse_articles_t_v2_articles_summary(objects: $objects, on_conflict: {constraint: t_v2_articles_summary_article_id_key}) {\n",
    "      affected_rows\n",
    "    }\n",
    "    update_synopse_articles_t_v1_rss1_articles_many(updates: $updates) {\n",
    "      affected_rows\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "  while True:\n",
    "    variables = {\n",
    "    \"limit\": 2,\n",
    "    \"offset\": offset\n",
    "    }\n",
    "    synopse_articles_t_v2_articles_summary_insert_input_loc = []\n",
    "    update_synopse_articles_t_v1_rss1_articles_many_loc = []\n",
    "    response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "    if len(response_data['data']['synopse_articles_t_v1_rss1_articles']) == 0:\n",
    "        break\n",
    "    for response in response_data['data']['synopse_articles_t_v1_rss1_articles']:\n",
    "      title = response['title']\n",
    "      summary = response['title']\n",
    "      article_id = response['id']\n",
    "      description = ', '.join(response['t_v1_rss1_articles_detail']['description'])\n",
    "      keyword_prompt = \"\"\"\n",
    "      [INST]\n",
    "      I have the following article that I'd like you to summarize:\n",
    "\n",
    "      Title: [TITLE]\n",
    "      Summary: [SUMMARY]\n",
    "      Description: [DESCRIPTION]\n",
    "\n",
    "      Please provide a concise summary of the article based on the information provided in the title, summary, and description. The summary should be approximately 300 words in length.\n",
    "      Make sure you to only return the summary and say nothing else. For example, don't say:\n",
    "      \"Here are the summary of the article\"\n",
    "      [/INST]\n",
    "      \"\"\"\n",
    "      # Replace placeholders with actual data\n",
    "      filled_prompt = keyword_prompt.replace(\"[TITLE]\", title).replace(\"[SUMMARY]\", summary).replace(\"[DESCRIPTION]\", description)\n",
    "      tokens = tokenizer.encode(filled_prompt)\n",
    "      token_count = len(tokens)\n",
    "      if token_count > 2000:\n",
    "        description_tokens = tokenizer.encode(description)\n",
    "        description_token_count = len(description_tokens)\n",
    "        req_tokens = description_token_count - (token_count - 2000)\n",
    "        tokens = tokenizer.encode(description)[:req_tokens]\n",
    "\n",
    "        # Decode the tokens back into a string\n",
    "        description = tokenizer.decode(tokens)\n",
    "        filled_prompt = keyword_prompt.replace(\"[TITLE]\", title).replace(\"[SUMMARY]\", summary).replace(\"[DESCRIPTION]\", description)\n",
    "      response1 = generator(filled_prompt)# Get the generated text\n",
    "      generated_text = response1[0][\"generated_text\"]\n",
    "      start_index = generated_text.find(\"[/INST]\") + len(\"[/INST]\")\n",
    "      summary = generated_text[start_index:].strip()\n",
    "      print(summary)\n",
    "      synopse_articles_t_v2_articles_summary_insert_input_loc.append({\n",
    "        \"article_id\": article_id,\n",
    "        \"summary\": summary,\n",
    "        }\n",
    "        )\n",
    "      update_synopse_articles_t_v1_rss1_articles_many_loc.append({\n",
    "        \"where\": {\"id\" : { \"_eq\": article_id }},\n",
    "        \"_set\": {\"is_summerized\": 1}\n",
    "        })\n",
    "    mutation_variables = {\n",
    "        \"objects\": synopse_articles_t_v2_articles_summary_insert_input_loc,\n",
    "        \"updates\": update_synopse_articles_t_v1_rss1_articles_many_loc,\n",
    "        }\n",
    "    out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "    print(out1)\n",
    "\n",
    "def ner_tagging(offset1):\n",
    "  query = \"\"\"\n",
    "  query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    synopse_articles_t_v1_rss1_articles(where: {is_summerized: {_eq: 1}, is_ner_tagged: {_eq: 0}}, limit: $limit, offset: $offset, order_by: {created_at: desc}) {\n",
    "      title\n",
    "      summary\n",
    "      id\n",
    "      t_v2_articles_summary {\n",
    "        summary\n",
    "        person_tags\n",
    "        location_tags\n",
    "        keywords_tags\n",
    "        org_tags\n",
    "      }\n",
    "      t_v1_rss1_articles_detail {\n",
    "        description\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "  offset = offset1\n",
    "  mutataion_query = \"\"\"\n",
    "  mutation MyMutation($updates: [synopse_articles_t_v2_articles_summary_updates!] = {where: {}}, $updates1: [synopse_articles_t_v1_rss1_articles_updates!] = {where: {}}) {\n",
    "    update_synopse_articles_t_v2_articles_summary_many(updates: $updates) {\n",
    "      affected_rows\n",
    "    }\n",
    "    update_synopse_articles_t_v1_rss1_articles_many(updates: $updates1) {\n",
    "      affected_rows\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "  while True:\n",
    "      variables = {\n",
    "          \"limit\": 2,\n",
    "          \"offset\": offset\n",
    "      }\n",
    "      synopse_articles_t_v2_articles_summary_updates_loc = []\n",
    "      synopse_articles_t_v1_rss1_articles_updates_loc = []\n",
    "      response_data = query_hasura_graphql(endpoint=endpoint, admin_key=admin_key, query=query, variables=variables)\n",
    "      if len(response_data['data']['synopse_articles_t_v1_rss1_articles']) == 0:\n",
    "          break\n",
    "      for response in response_data['data']['synopse_articles_t_v1_rss1_articles']:\n",
    "          title = response['title']\n",
    "          summary = response['summary']\n",
    "          summary2 = response['t_v2_articles_summary']['summary']\n",
    "          description = \", \".join(response['t_v1_rss1_articles_detail']['description'])\n",
    "          text = title + \" \" + summary + \" \" + summary2 + \" \" + description\n",
    "          doc = nlp(text)\n",
    "          kw_model = KeyBERT()\n",
    "          num_words = len(text.split())\n",
    "          top_keywords = int(num_words/50)\n",
    "          if top_keywords < 15:\n",
    "              top_keywords = 15\n",
    "          elif top_keywords > 30:\n",
    "              top_keywords = 30\n",
    "          if response['t_v2_articles_summary']['person_tags'] is None:\n",
    "              person = []\n",
    "          else:\n",
    "              person = response['t_v2_articles_summary']['person_tags']\n",
    "          if response['t_v2_articles_summary']['location_tags'] is None:\n",
    "              loc = []\n",
    "          else:\n",
    "              loc = response['t_v2_articles_summary']['location_tags']\n",
    "          if response['t_v2_articles_summary']['keywords_tags'] is None:\n",
    "              keys = []\n",
    "          else:\n",
    "              keys = response['t_v2_articles_summary']['keywords_tags']\n",
    "          if response['t_v2_articles_summary']['org_tags'] is None:\n",
    "              orgs = []\n",
    "          else:\n",
    "              orgs = response['t_v2_articles_summary']['org_tags']\n",
    "          keywords = kw_model.extract_keywords(text, top_n=int(top_keywords))\n",
    "          keys_bert = [keyword[0] for keyword in keywords]\n",
    "          for key in keys_bert:\n",
    "              keys.append(key)\n",
    "          for ent in doc.ents:\n",
    "              if ent.label_ == \"GPE\" or ent.label_ == \"LOC\" or ent.label_ == \"NORP\" or ent.label_ == \"FAC\":\n",
    "                  loc.append(ent.text)\n",
    "              elif ent.label_ == \"PERSON\":\n",
    "                  person.append(ent.text)\n",
    "              elif ent.label_ == \"ORG \":\n",
    "                  orgs.append(ent.text)\n",
    "              elif ent.label_ == \"EVENT\" or ent.label_ == \"WORK_OF_ART\" or ent.label_ == \"PRODUCT\" or ent.label_ == \"LAW\":\n",
    "                  keys.append(ent.text)\n",
    "          loc = [item.lower() for item in loc]\n",
    "          person = [item.lower() for item in person]\n",
    "          key = [item.lower() for item in keys]\n",
    "          org = [item.lower() for item in orgs]\n",
    "          loc = list(set(loc))\n",
    "          person = list(set(person))\n",
    "          key = list(set(key))\n",
    "          org = list(set(org))\n",
    "          synopse_articles_t_v2_articles_summary_updates_loc.append({\n",
    "                  \"where\": {\"article_id\" : { \"_eq\": response['id'] }},\n",
    "                  \"_set\": {\"location_tags\": loc, \"person_tags\": person, \"keywords_tags\": key , \"org_tags\": org}\n",
    "                  })\n",
    "          synopse_articles_t_v1_rss1_articles_updates_loc.append({\n",
    "                  \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                  \"_set\": {\"is_ner_tagged\": 1}\n",
    "                  })\n",
    "      mutation_variables = {\n",
    "          \"updates\": synopse_articles_t_v2_articles_summary_updates_loc,\n",
    "          \"updates1\": synopse_articles_t_v1_rss1_articles_updates_loc\n",
    "      }\n",
    "      out = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutataion_query, mutation_variables=mutation_variables)\n",
    "\n",
    "\n",
    "def ai_tagging(offset1):\n",
    "  device = 0 if torch.cuda.is_available() else -1  # Use GPU if available, else CPU\n",
    "  print(device)\n",
    "  classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)\n",
    "  graphql_tags_query = '''\n",
    "  query MyQuery {\n",
    "    synopse_articles_t_v4_tags_hierarchy(where: {is_valid: {_eq: 1}}) {\n",
    "      id\n",
    "      tag\n",
    "      tag_hierachy\n",
    "    }\n",
    "  }\n",
    "  '''\n",
    "  tags_main = []\n",
    "  tags_ids = []\n",
    "  variables_tags = {\n",
    "  }\n",
    "  response_data_tags = query_hasura_graphql(endpoint, admin_key, graphql_tags_query, variables_tags)\n",
    "  tags_main = []\n",
    "  tags_all = []\n",
    "  for response in response_data_tags['data']['synopse_articles_t_v4_tags_hierarchy']:\n",
    "    if response['tag_hierachy'] == 0:\n",
    "      tags_main.append(response['tag'])\n",
    "      tags_ids.append(response['id'])\n",
    "    else:\n",
    "      tags_all.append(response['tag'])\n",
    "      #print(response['tag'], response['id'])\n",
    "  tags_all = list(set(tags_all))\n",
    "  tags_l1 = \", \".join(tags_main)\n",
    "  query = \"\"\"\n",
    "  query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    synopse_articles_t_v1_rss1_articles(where: {is_summerized: {_eq: 1}, is_ner_tagged: {_eq: 1}, is_ai_tagged: {_eq: 0}}, limit: $limit, offset: $offset, order_by: {created_at: desc}) {\n",
    "      title\n",
    "      summary\n",
    "      id\n",
    "      t_v2_articles_summary {\n",
    "        summary\n",
    "        person_tags\n",
    "        location_tags\n",
    "        keywords_tags\n",
    "        org_tags\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "  offset = offset1\n",
    "  mutataion_query = \"\"\"\n",
    "  mutation MyMutation($updates: [synopse_articles_t_v2_articles_summary_updates!] = {where: {}}, $updates1: [synopse_articles_t_v1_rss1_articles_updates!] = {where: {}}) {\n",
    "    update_synopse_articles_t_v2_articles_summary_many(updates: $updates) {\n",
    "      affected_rows\n",
    "    }\n",
    "    update_synopse_articles_t_v1_rss1_articles_many(updates: $updates1) {\n",
    "      affected_rows\n",
    "    }\n",
    "  }\n",
    "    \"\"\"\n",
    "  while True:\n",
    "      variables = {\n",
    "          \"limit\": 2,\n",
    "          \"offset\": offset\n",
    "      }\n",
    "      synopse_articles_t_v2_articles_summary_updates_loc = []\n",
    "      synopse_articles_t_v1_rss1_articles_updates_loc = []\n",
    "      response_data = query_hasura_graphql(endpoint=endpoint, admin_key=admin_key, query=query, variables=variables)\n",
    "      if len(response_data['data']['synopse_articles_t_v1_rss1_articles']) == 0:\n",
    "          break\n",
    "      for response in response_data['data']['synopse_articles_t_v1_rss1_articles']:\n",
    "          title = response['title']\n",
    "          summary = response['summary']\n",
    "          summary2 = response['t_v2_articles_summary']['summary']\n",
    "          if response['t_v2_articles_summary']['person_tags'] is None:\n",
    "              person_tags = \" \"\n",
    "          else:\n",
    "              person_tags = \", \".join(response['t_v2_articles_summary']['person_tags'])\n",
    "          if response['t_v2_articles_summary']['location_tags'] is None:\n",
    "              location_tags = \" \"\n",
    "          else:\n",
    "              location_tags = \", \".join(response['t_v2_articles_summary']['location_tags'])\n",
    "          if response['t_v2_articles_summary']['keywords_tags'] is None:\n",
    "              keywords_tags = \" \"\n",
    "          else:\n",
    "              keywords_tags =  \", \".join(response['t_v2_articles_summary']['keywords_tags'])\n",
    "          if response['t_v2_articles_summary']['org_tags'] is None:\n",
    "              org_tags = \" \"\n",
    "          else:\n",
    "              org_tags =  \", \".join(response['t_v2_articles_summary']['org_tags'])\n",
    "          text = title + \" \" + summary + \" \" + summary2 + \" \" + person_tags + \" \" + location_tags + \" \" + keywords_tags + \" \" + org_tags\n",
    "          sequence_to_classify = text\n",
    "          candidate_labels = tags_l1\n",
    "          cls = classifier(sequence_to_classify, candidate_labels, multi_label=True)\n",
    "          article_tags = []\n",
    "          article_tags.append(cls['labels'][0])\n",
    "          i1 = tags_main.index(article_tags[0])\n",
    "          t1_index = tags_ids[i1]\n",
    "          article_tags.append(cls['labels'][1])\n",
    "          i2 = tags_main.index(article_tags[1])\n",
    "          t2_index = tags_ids[i2]\n",
    "          tags_sub=[]\n",
    "          for responset in response_data_tags['data']['synopse_articles_t_v4_tags_hierarchy']:\n",
    "            if responset['tag_hierachy'] == t1_index:\n",
    "              tags_sub.append(responset['tag'])\n",
    "          tags_sub = \", \".join(tags_sub)\n",
    "          cls = classifier(sequence_to_classify, tags_sub, multi_label=True)\n",
    "          article_tags.append(cls['labels'][0])\n",
    "          article_tags.append(cls['labels'][1])\n",
    "          tags_sub=[]\n",
    "          for responset in response_data_tags['data']['synopse_articles_t_v4_tags_hierarchy']:\n",
    "            if responset['tag_hierachy'] == t2_index:\n",
    "              tags_sub.append(responset['tag'])\n",
    "          tags_sub = \", \".join(tags_sub)\n",
    "          cls = classifier(sequence_to_classify, tags_sub, multi_label=True)\n",
    "          article_tags.append(cls['labels'][0])\n",
    "          article_tags.append(cls['labels'][1])\n",
    "          synopse_articles_t_v2_articles_summary_updates_loc.append({\n",
    "                  \"where\": {\"article_id\" : { \"_eq\": response['id'] }},\n",
    "                  \"_set\": {\"ai_tags\": article_tags}\n",
    "                  })\n",
    "          synopse_articles_t_v1_rss1_articles_updates_loc.append({\n",
    "                  \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                  \"_set\": {\"is_ai_tagged\": 1}\n",
    "                  })\n",
    "      mutation_variables = {\n",
    "          \"updates\": synopse_articles_t_v2_articles_summary_updates_loc,\n",
    "          \"updates1\": synopse_articles_t_v1_rss1_articles_updates_loc\n",
    "      }\n",
    "      out = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutataion_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def vectorize(offset1):\n",
    "  model = SentenceTransformer('BAAI/bge-large-zh-v1.5')\n",
    "  graphql_query = '''\n",
    "  query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    synopse_articles_t_v1_rss1_articles(limit: $limit, offset: $offset, where: {is_summerized: {_eq: 1}, is_vectorized: {_eq: 0}, is_ai_tagged: {_eq: 1}}, order_by: {created_at: desc}) {\n",
    "      id\n",
    "      title\n",
    "      summary\n",
    "      t_v2_articles_summary {\n",
    "        summary\n",
    "        keywords_tags\n",
    "        location_tags\n",
    "        org_tags\n",
    "        person_tags\n",
    "        ai_tags\n",
    "      }\n",
    "      tags\n",
    "    }\n",
    "  }\n",
    "  '''\n",
    "  offset = offset1\n",
    "  mutation_query = \"\"\"\n",
    "  mutation MyMutation($objects: [synopse_articles_t_v2_articles_vectors_insert_input!] = {}, $updates: [synopse_articles_t_v1_rss1_articles_updates!] = {where: {}}) {\n",
    "    insert_synopse_articles_t_v2_articles_vectors(objects: $objects, on_conflict: {constraint: t_v2_articles_vectors_article_id_key}) {\n",
    "      affected_rows\n",
    "    }\n",
    "    update_synopse_articles_t_v1_rss1_articles_many(updates: $updates) {\n",
    "      affected_rows\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "  while True:\n",
    "    variables = {\n",
    "    \"limit\": 20,\n",
    "    \"offset\": offset\n",
    "    }\n",
    "    synopse_articles_t_v2_articles_vectors_insert_input_loc = []\n",
    "    synopse_articles_t_v1_rss1_articles_updates_loc = []\n",
    "    response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "    if len(response_data['data']['synopse_articles_t_v1_rss1_articles']) == 0:\n",
    "        break\n",
    "    p1 = []\n",
    "    article_ids = []\n",
    "    for response in response_data['data']['synopse_articles_t_v1_rss1_articles']:\n",
    "      article_ids.append( response['id'] )\n",
    "      tags  = ', '.join(response['tags'] ) + \" \" + ', '.join(response['t_v2_articles_summary']['keywords_tags'] ) + \" \" + ', '.join(response['t_v2_articles_summary']['location_tags'] ) + \" \" + ', '.join(response['t_v2_articles_summary']['org_tags'] ) + \" \" + ', '.join(response['t_v2_articles_summary']['person_tags'] ) + \" \" + ', '.join(response['t_v2_articles_summary']['ai_tags'] ) + \" \" + ', '.join(response['t_v2_articles_summary']['ai_tags'] )\n",
    "      p12 = response['title'] + \"\\n\" + response['summary'] + \"\\n\" + response['t_v2_articles_summary']['summary'] + tags\n",
    "      p1.append(p12)\n",
    "\n",
    "    embeddings = model.encode(p1)\n",
    "    for i in range(0,len(article_ids)):\n",
    "      synopse_articles_t_v2_articles_vectors_insert_input_loc.append({\n",
    "          \"article_id\": article_ids[i],\n",
    "          \"a_vector\":  str(embeddings[i].tolist()),\n",
    "          }\n",
    "          )\n",
    "      synopse_articles_t_v1_rss1_articles_updates_loc.append({\n",
    "          \"where\": {\"id\" : { \"_eq\": article_ids[i] }},\n",
    "          \"_set\": {\"is_vectorized\": 1}\n",
    "          })\n",
    "\n",
    "    mutation_variables = {\n",
    "        \"objects\": synopse_articles_t_v2_articles_vectors_insert_input_loc,\n",
    "        \"updates\": synopse_articles_t_v1_rss1_articles_updates_loc,\n",
    "        }\n",
    "    out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def grouping_l1(offset1):\n",
    "    graphql_query = '''\n",
    "    query MyQuery($offset: Int!, $limit: Int!) {\n",
    "      synopse_articles_t_v1_rss1_articles(offset: $offset, limit: $limit, order_by: {created_at: desc}, where: {is_grouped: {_eq: 0}, is_vectorized: {_eq: 1}}) {\n",
    "        id\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [synopse_articles_t_v3_article_groups_l1_insert_input!] = {}, $updates: [synopse_articles_t_v1_rss1_articles_updates!] = {where: {}}) {\n",
    "      insert_synopse_articles_t_v3_article_groups_l1(objects: $objects, on_conflict: {constraint: t_v3_article_groups_l1_article_id_key}) {\n",
    "        affected_rows\n",
    "      }\n",
    "      update_synopse_articles_t_v1_rss1_articles_many(updates: $updates) {\n",
    "        affected_rows\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    func_query = '''\n",
    "    query MyQuery($p_article_id: bigint!) {\n",
    "      synopse_articles_f_get_similar_articles_group(args: {p_article_id: $p_article_id}) {\n",
    "        article_id\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 20,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        synopse_articles_t_v3_article_groups_l1_insert_input_loc=[]\n",
    "        synopse_articles_t_v1_rss1_articles_updates_loc=[]\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['synopse_articles_t_v1_rss1_articles']) == 0:\n",
    "            break\n",
    "        s1= []\n",
    "        ids=[]\n",
    "        for response in response_data['data']['synopse_articles_t_v1_rss1_articles']:\n",
    "            func_variables = {\n",
    "                \"p_article_id\": response['id']\n",
    "                }\n",
    "            func_response_data = query_hasura_graphql(endpoint, admin_key, func_query, func_variables)\n",
    "            article_group = []\n",
    "            #print(json.dumps(func_response_data, indent=4))\n",
    "            if len(func_response_data['data']['synopse_articles_f_get_similar_articles_group']) > 0:\n",
    "                for func_response in func_response_data['data']['synopse_articles_f_get_similar_articles_group']:\n",
    "                    article_group.append(func_response['article_id'])\n",
    "\n",
    "            synopse_articles_t_v3_article_groups_l1_insert_input_loc.append({\n",
    "                \"article_id\": response['id'],\n",
    "                \"initial_group\": article_group,\n",
    "                \"article_count\": len(article_group)\n",
    "                }\n",
    "                )\n",
    "            synopse_articles_t_v1_rss1_articles_updates_loc.append({\n",
    "                \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                \"_set\": {\"is_grouped\": 1}\n",
    "                })\n",
    "        mutation_variables = {\n",
    "        \"objects\": synopse_articles_t_v3_article_groups_l1_insert_input_loc,\n",
    "        \"updates\": synopse_articles_t_v1_rss1_articles_updates_loc,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def grouping_l2(offset1):\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "      synopse_articles_t_v3_article_groups_l1(where: {t_v1_rss1_article: {is_grouped: {_eq: 1}}, article_count: {_gt: 1}}, order_by: {updated_at: asc}, limit: $limit, offset: $offset) {\n",
    "        article_id\n",
    "        initial_group\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    query2 = '''\n",
    "    query MyQuery($articleid: [bigint!] = []) {\n",
    "        synopse_articles_t_v3_article_groups_l1(where: {initial_group: {_contains: $articleid}}) {\n",
    "        article_id\n",
    "        initial_group\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    query3 = '''\n",
    "    query MyQuery($articleid: [bigint!] = []) {\n",
    "        synopse_articles_t_v3_article_groups_l2(where: {articles_group: {_contains: $articleid}}, order_by: {is_valid: desc, updated_at: desc}) {\n",
    "        id\n",
    "        articles_group\n",
    "        is_valid\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [synopse_articles_t_v3_article_groups_l2_insert_input!] = {}, $updates: [synopse_articles_t_v1_rss1_articles_updates!] = {where: {}}, $updates1: [synopse_articles_t_v3_article_groups_l2_updates!] = {where: {}}, $articleGroupIds: [bigint!] = \"\") {\n",
    "      insert_synopse_articles_t_v3_article_groups_l2(objects: $objects, on_conflict: {constraint: t_v3_article_groups_l2_pkey}) {\n",
    "        affected_rows\n",
    "      }\n",
    "      update_synopse_articles_t_v1_rss1_articles_many(updates: $updates) {\n",
    "        affected_rows\n",
    "      }\n",
    "      update_synopse_articles_t_v3_article_groups_l2_many(updates: $updates1) {\n",
    "        affected_rows\n",
    "      }\n",
    "      delete_synopse_articles_t_v3_article_groups_l2(where: {id: {_in: $articleGroupIds}}) {\n",
    "        affected_rows\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": 0\n",
    "        }\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        synopse_articles_t_v3_article_groups_l2_insert_input_loc=[]\n",
    "        synopse_articles_t_v1_rss1_articles_updates_loc=[]\n",
    "        synopse_articles_t_v3_article_groups_l2_updates_loc=[]\n",
    "        articleGroupIds_loc=[]\n",
    "        if len(response_data['data']['synopse_articles_t_v3_article_groups_l1']) == 0:\n",
    "            break\n",
    "        for response in response_data['data']['synopse_articles_t_v3_article_groups_l1']:\n",
    "            variables2 = {\n",
    "                \"articleid\": [response['article_id']]\n",
    "                }\n",
    "            response_data1 = query_hasura_graphql(endpoint, admin_key, query2, variables2)\n",
    "            articles_ids = []\n",
    "            if len(response_data1['data']['synopse_articles_t_v3_article_groups_l1']) > 0:\n",
    "                for func_response in response_data1['data']['synopse_articles_t_v3_article_groups_l1']:\n",
    "                    articles_ids.append(func_response['initial_group'])\n",
    "            n1 = []\n",
    "            for sublist in articles_ids:\n",
    "                for element in sublist:\n",
    "                    n1.append(element)\n",
    "            articles_ids = list(set(n1))\n",
    "            articles_ids.sort(reverse=True)\n",
    "            while True:\n",
    "                if len(articles_ids) > 1:\n",
    "                    articles_news = []\n",
    "                    for article_id in articles_ids:\n",
    "                        variables3 = {\n",
    "                            \"articleid\": [article_id]\n",
    "                            }\n",
    "                        response_data2 = query_hasura_graphql(endpoint, admin_key, query2, variables3)\n",
    "                        if len(response_data1['data']['synopse_articles_t_v3_article_groups_l1']) > 0:\n",
    "                            for func_response in response_data1['data']['synopse_articles_t_v3_article_groups_l1']:\n",
    "                                articles_news.append(func_response['initial_group'])\n",
    "                    n2 = []\n",
    "                    for sublist in articles_news:\n",
    "                        for element in sublist:\n",
    "                            n2.append(element)\n",
    "                    articles_news = list(set(n2))\n",
    "                    articles_news.sort(reverse=True)\n",
    "                    if articles_ids == articles_news:\n",
    "                        break\n",
    "                    else:\n",
    "                        articles_ids = articles_news\n",
    "            n5 = []\n",
    "            for a1 in articles_ids:\n",
    "                variables3 = {\n",
    "                    \"articleid\": [a1]\n",
    "                    }\n",
    "                response_data2 = query_hasura_graphql(endpoint, admin_key, query3, variables3)\n",
    "                if len(response_data2['data']['synopse_articles_t_v3_article_groups_l2']) > 0:\n",
    "                    for func_response in response_data2['data']['synopse_articles_t_v3_article_groups_l2']:\n",
    "                        n5.append(func_response['articles_group'])\n",
    "                        if func_response['is_valid'] == 0:\n",
    "                            articleGroupIds_loc.append(func_response['id'])\n",
    "            n5.append(articles_ids)\n",
    "            n2 = []\n",
    "            for sublist in n5:\n",
    "                for element in sublist:\n",
    "                    n2.append(element)\n",
    "            articles_ids = list(set(n2))\n",
    "            articles_ids.sort(reverse=True)\n",
    "            synopse_articles_t_v3_article_groups_l2_insert_input_loc.append({\n",
    "                \"articles_group\": articles_ids,\n",
    "                'articles_in_group': len(articles_ids)\n",
    "                }\n",
    "                )\n",
    "            for article in articles_ids:\n",
    "                synopse_articles_t_v1_rss1_articles_updates_loc.append({\n",
    "                    \"where\": {\"id\" : { \"_eq\": article }},\n",
    "                    \"_set\": {\"is_grouped\": 2}\n",
    "                    })\n",
    "        mutation_variables = {\n",
    "            \"objects\": synopse_articles_t_v3_article_groups_l2_insert_input_loc,\n",
    "            \"updates\": synopse_articles_t_v1_rss1_articles_updates_loc,\n",
    "            \"updates1\": synopse_articles_t_v3_article_groups_l2_updates_loc,\n",
    "            \"articleGroupIds\": articleGroupIds_loc\n",
    "            }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "\n",
    "def gen_article(offset1):\n",
    "    # Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "        model_file=\"mistral-7b-instruct-v0.1.Q8_0.gguf\",\n",
    "        model_type=\"mistral\",\n",
    "        gpu_layers=110,\n",
    "        hf=True,\n",
    "        context_length=8000\n",
    "    )\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "    # Pipeline\n",
    "    generator = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task='text-generation',\n",
    "        max_new_tokens=6000,\n",
    "        min_new_tokens=400,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "      synopse_articles_t_v3_article_groups_l2(where: {is_valid: {_eq: 0}, is_summerized: {_eq: 0}, articles_in_group: {_gte: 2}}, offset: $offset, limit: $limit, order_by: {created_at: desc}) {\n",
    "        articles_group\n",
    "        id\n",
    "      }\n",
    "    }\n",
    "\n",
    "    '''\n",
    "    graphql_query_article = '''\n",
    "    query MyQuery($articles: [bigint!] = []) {\n",
    "      synopse_articles_t_v1_rss1_articles(where: {id: {_in: $articles}}, order_by: {created_at: desc}) {\n",
    "        title\n",
    "        summary\n",
    "        t_v2_articles_summary {\n",
    "          summary\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [synopse_articles_t_v4_article_groups_l2_detail_insert_input!] = {}, $updates: [synopse_articles_t_v3_article_groups_l2_updates!] = {where: {}}) {\n",
    "      insert_synopse_articles_t_v4_article_groups_l2_detail(objects: $objects, on_conflict: {constraint: t_v4_article_groups_l2_detail_article_group_id_key}) {\n",
    "        affected_rows\n",
    "      }\n",
    "      update_synopse_articles_t_v3_article_groups_l2_many(updates: $updates) {\n",
    "        affected_rows\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['synopse_articles_t_v3_article_groups_l2']) == 0:\n",
    "            break\n",
    "        for response in response_data['data']['synopse_articles_t_v3_article_groups_l2']:\n",
    "            llm_text = ''\n",
    "            synopse_articles_t_v4_article_groups_l2_detail_insert_input_loc=[]\n",
    "            synopse_articles_t_v3_article_groups_l2_updates_loc=[]\n",
    "            variables1 = {\n",
    "            \"articles\": response['articles_group']\n",
    "            }\n",
    "            response_data1 = query_hasura_graphql(endpoint, admin_key, graphql_query_article, variables1)\n",
    "            titles = \"\"\n",
    "            summarys = \"\"\n",
    "            summarys_detailed = \"\"\n",
    "            for response1 in response_data1['data']['synopse_articles_t_v1_rss1_articles']:\n",
    "                titles = titles + response1['title']\n",
    "                summarys =summarys + response1['summary']\n",
    "                summarys_detailed = summarys_detailed + response1['t_v2_articles_summary']['summary']\n",
    "            keyword_prompt = \"\"\"\n",
    "            [INST]\n",
    "            I have the following article that I'd like you to summarize:\n",
    "\n",
    "            Titles: [TITLE]\n",
    "            Summarys: [SUMMARY]\n",
    "            Summarys_detailed: [SUMMARYDETAILED]\n",
    "\n",
    "\n",
    "            Please provide a concise UNBIASED news article based on the information provided in the title, Summarys, and Summarys_detailed.\n",
    "            The summary should be approximately 500 words in length.\n",
    "            Make sure you to only return the article wit atleast 3 paragraphsand say nothing else. For example, don't say:\n",
    "            \"Here are the article title or article\"\n",
    "            [/INST]\n",
    "            \"\"\"\n",
    "            # Replace placeholders with actual data\n",
    "            filled_prompt = keyword_prompt.replace(\"[TITLE]\", titles).replace(\"[SUMMARY]\", summarys).replace(\"[SUMMARYDETAILED]\", summarys_detailed)\n",
    "            tokens = tokenizer.encode(filled_prompt)\n",
    "            token_count = len(tokens)\n",
    "            if token_count > 5000:\n",
    "              filled_prompt = keyword_prompt.replace(\"[TITLE]\", titles).replace(\"[SUMMARY]\", \"\").replace(\"[SUMMARYDETAILED]\", summarys_detailed)\n",
    "            tokens = tokenizer.encode(filled_prompt)\n",
    "            token_count = len(tokens)\n",
    "            if token_count > 5000:\n",
    "              summarys_detailed_tokens = tokenizer.encode(summarys_detailed)\n",
    "              summarys_detailed_token_count = len(summarys_detailed_tokens)\n",
    "              req_tokens = summarys_detailed_token_count - (token_count - 5000)\n",
    "              tokens = tokenizer.encode(summarys_detailed)[:req_tokens]\n",
    "\n",
    "              # Decode the tokens back into a string\n",
    "              summarys_detailed = tokenizer.decode(tokens)\n",
    "              filled_prompt = keyword_prompt.replace(\"[TITLE]\", titles).replace(\"[SUMMARY]\", summarys).replace(\"[SUMMARYDETAILED]\", summarys_detailed)\n",
    "            #print(filled_prompt)\n",
    "\n",
    "            response1 = generator(filled_prompt)# Get the generated text\n",
    "            generated_text = response1[0][\"generated_text\"]\n",
    "            start_index = generated_text.find(\"[/INST]\") + len(\"[/INST]\")\n",
    "            article_final = generated_text[start_index:].strip()\n",
    "            print(article_final)\n",
    "            synopse_articles_t_v4_article_groups_l2_detail_insert_input_loc.append({\n",
    "              \"article_group_id\": response['id'],\n",
    "              \"summary\": article_final,\n",
    "              }\n",
    "              )\n",
    "            synopse_articles_t_v3_article_groups_l2_updates_loc.append({\n",
    "              \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "              \"_set\": {\"is_summerized\": 1 , \"is_valid\": 1}\n",
    "              })\n",
    "        mutation_variables = {\n",
    "        \"objects\": synopse_articles_t_v4_article_groups_l2_detail_insert_input_loc,\n",
    "        \"updates\": synopse_articles_t_v3_article_groups_l2_updates_loc,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def gen_title(offset1):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "      synopse_articles_t_v4_article_groups_l2_detail(where: {title: {_is_null: true}}, limit: $limit, offset: $offset, order_by: {created_at: desc}) {\n",
    "        summary\n",
    "        article_group_id\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($updates: [synopse_articles_t_v4_article_groups_l2_detail_updates!] = {where: {}}) {\n",
    "      update_synopse_articles_t_v4_article_groups_l2_detail_many(updates: $updates) {\n",
    "        affected_rows\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['synopse_articles_t_v4_article_groups_l2_detail']) == 0:\n",
    "            break\n",
    "        for response in response_data['data']['synopse_articles_t_v4_article_groups_l2_detail']:\n",
    "            llm_text = ''\n",
    "            synopse_articles_t_v4_article_groups_l2_detail_updates_loc=[]\n",
    "            input_text = \"generate a intresting and viral news title for:  \" + \"\\n\" + response['summary']\n",
    "            max_length = 512\n",
    "            input_text = input_text[:max_length]\n",
    "            input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "            outputs = model.generate(input_ids, max_new_tokens=50)\n",
    "            generated_text = tokenizer.decode(outputs[0])\n",
    "            clean_text = re.sub('<.*?>', '', generated_text) # remove data between < and >\n",
    "            print(clean_text)\n",
    "            synopse_articles_t_v4_article_groups_l2_detail_updates_loc.append({\n",
    "              \"where\": {\"article_group_id\" : { \"_eq\": response['article_group_id'] }},\n",
    "              \"_set\": {\"title\": clean_text }\n",
    "              })\n",
    "        mutation_variables = {\n",
    "        \"updates\": synopse_articles_t_v4_article_groups_l2_detail_updates_loc,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "\n",
    "def gen_summary_60_words(offset1):\n",
    "     # Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "        model_file=\"mistral-7b-instruct-v0.1.Q8_0.gguf\",\n",
    "        model_type=\"mistral\",\n",
    "        gpu_layers=110,\n",
    "        hf=True,\n",
    "        context_length=8000\n",
    "    )\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "    # Pipeline\n",
    "    generator = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task='text-generation',\n",
    "        max_new_tokens=6000,\n",
    "        min_new_tokens=80,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "      synopse_articles_t_v4_article_groups_l2_detail(where: {summary_60_words: {_is_null: true}, title: {_is_null: false}, summary: {_is_null: false}}, limit: $limit, offset: $offset) {\n",
    "        article_group_id\n",
    "        title\n",
    "        summary\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($updates: [synopse_articles_t_v4_article_groups_l2_detail_updates!] = {where: {}}) {\n",
    "      update_synopse_articles_t_v4_article_groups_l2_detail_many(updates: $updates) {\n",
    "        affected_rows\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['synopse_articles_t_v4_article_groups_l2_detail']) == 0:\n",
    "            break\n",
    "        for response in response_data['data']['synopse_articles_t_v4_article_groups_l2_detail']:\n",
    "\n",
    "            synopse_articles_t_v4_article_groups_l2_detail_updates_loc=[]\n",
    "            keyword_prompt = \"\"\"\n",
    "            [INST]\n",
    "            I have the following article that I'd like you to summarize:\n",
    "\n",
    "            Title: [TITLE]\n",
    "            detail: [SUMMARY]\n",
    "\n",
    "\n",
    "            Please provide a concise UNBIASED news summary article based on the information provided in the Title, and detail.\n",
    "            The summary should be approximately 70 words in length.\n",
    "            Make sure you to only return the article say nothing else. For example, don't say:\n",
    "            \"Here are the article title or article\"\n",
    "            [/INST]\n",
    "            \"\"\"\n",
    "            # Replace placeholders with actual data\n",
    "            filled_prompt = keyword_prompt.replace(\"[TITLE]\", response['title']).replace(\"[SUMMARY]\",  response['summary'])\n",
    "            response1 = generator(filled_prompt)# Get the generated text\n",
    "            generated_text = response1[0][\"generated_text\"]\n",
    "            start_index = generated_text.find(\"[/INST]\") + len(\"[/INST]\")\n",
    "            article_final = generated_text[start_index:].strip()\n",
    "            print(article_final)\n",
    "            synopse_articles_t_v4_article_groups_l2_detail_updates_loc.append({\n",
    "              \"where\": {\"article_group_id\" : { \"_eq\": response['article_group_id'] }},\n",
    "              \"_set\": {\"summary_60_words\": article_final }\n",
    "              })\n",
    "        mutation_variables = {\n",
    "        \"updates\": synopse_articles_t_v4_article_groups_l2_detail_updates_loc,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def array2dto2d(n2):\n",
    "    n1 = []\n",
    "    for sublist in n2:\n",
    "        for element in sublist:\n",
    "            n1.append(element)\n",
    "    n2 = list(set(n1))\n",
    "    return n2\n",
    "\n",
    "def detail_tags(offset1):\n",
    "  graphql_query = '''\n",
    "  query MyQuery($limit: Int!, $offset: Int!) {\n",
    "  synopse_articles_t_v4_article_groups_l2_detail(where: {ai_tags: {_is_null: true}, summary_60_words: {_is_null: false}}, limit: $limit, offset: $offset) {\n",
    "      id\n",
    "      t_v3_article_groups_l2 {\n",
    "        articles_group\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  '''\n",
    "  query2 = '''\n",
    "  query MyQuery($article_id: bigint = \"\") {\n",
    "    synopse_articles_t_v1_rss1_articles(where: {id: {_eq: $article_id}}) {\n",
    "      image_link\n",
    "      tags\n",
    "      t_v1_rss1_feed_link {\n",
    "        t_v1_outlet {\n",
    "          logo_url\n",
    "        }\n",
    "      }\n",
    "      t_v2_articles_summary {\n",
    "        keywords_tags\n",
    "        location_tags\n",
    "        org_tags\n",
    "        person_tags\n",
    "        ai_tags\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  '''\n",
    "  offset = offset1\n",
    "  mutation_query = \"\"\"\n",
    "  mutation MyMutation($updates: [synopse_articles_t_v4_article_groups_l2_detail_updates!] = {where: {}}) {\n",
    "    update_synopse_articles_t_v4_article_groups_l2_detail_many(updates: $updates) {\n",
    "      affected_rows\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "  while True:\n",
    "      variables = {\n",
    "      \"limit\": 1,\n",
    "      \"offset\": offset\n",
    "      }\n",
    "      response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "      synopse_articles_t_v4_article_groups_l2_detail_updates_loc=[]\n",
    "      image_urls=[]\n",
    "      logo_urls=[]\n",
    "      keywords_tags=[]\n",
    "      location_tags=[]\n",
    "      org_tags=[]\n",
    "      person_tags=[]\n",
    "      ai_tags=[]\n",
    "      if len(response_data['data']['synopse_articles_t_v4_article_groups_l2_detail']) == 0:\n",
    "            break\n",
    "      for response in response_data['data']['synopse_articles_t_v4_article_groups_l2_detail']:\n",
    "          for article in response['t_v3_article_groups_l2']['articles_group']:\n",
    "              variables2 = {\n",
    "              \"article_id\": article\n",
    "              }\n",
    "              response_data1 = query_hasura_graphql(endpoint, admin_key, query2, variables2)\n",
    "              for response1 in response_data1['data']['synopse_articles_t_v1_rss1_articles']:\n",
    "                  image_urls.append(response1['image_link'])\n",
    "                  logo_urls.append(response1['t_v1_rss1_feed_link']['t_v1_outlet']['logo_url'])\n",
    "                  keywords_tags.append(response1['tags'])\n",
    "                  keywords_tags.append(response1['t_v2_articles_summary']['keywords_tags'])\n",
    "                  location_tags.append(response1['t_v2_articles_summary']['location_tags'])\n",
    "                  org_tags.append(response1['t_v2_articles_summary']['org_tags'])\n",
    "                  person_tags.append(response1['t_v2_articles_summary']['person_tags'])\n",
    "                  ai_tags.append(response1['t_v2_articles_summary']['ai_tags'])\n",
    "      image_urls = list(set(image_urls))\n",
    "      image_urls = [url for url in image_urls if url != \"\"]\n",
    "      logo_urls = list(set(logo_urls))\n",
    "      logo_urls = [url for url in logo_urls if url != \"\"]\n",
    "      keywords_tags = array2dto2d(keywords_tags)\n",
    "      location_tags = array2dto2d(location_tags)\n",
    "      org_tags = array2dto2d(org_tags)\n",
    "      person_tags = array2dto2d(person_tags)\n",
    "      ai_tags = array2dto2d(ai_tags)\n",
    "      synopse_articles_t_v4_article_groups_l2_detail_updates_loc.append({\n",
    "          \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "          \"_set\": {\"image_urls\": image_urls,\n",
    "                  \"logo_urls\": logo_urls,\n",
    "                  \"keywords_tags\": keywords_tags,\n",
    "                  \"location_tags\": location_tags,\n",
    "                  \"org_tags\": org_tags,\n",
    "                  \"person_tags\": person_tags,\n",
    "                  \"ai_tags\": ai_tags\n",
    "                   }\n",
    "          })\n",
    "      mutation_variables = {\n",
    "      \"updates\": synopse_articles_t_v4_article_groups_l2_detail_updates_loc,\n",
    "      }\n",
    "      out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def vectorize_groups(offset1):\n",
    "  model = SentenceTransformer('BAAI/bge-large-zh-v1.5')\n",
    "  graphql_query = '''\n",
    "  query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    synopse_articles_t_v3_article_groups_l2(where: {is_vectorized: {_eq: 0}, is_summerized: {_eq: 1}, t_v4_article_groups_l2_detail: {ai_tags: {_is_null: false}}}, limit: $limit, offset: $offset) {\n",
    "      t_v4_article_groups_l2_detail {\n",
    "        article_group_id\n",
    "        keywords_tags\n",
    "        location_tags\n",
    "        org_tags\n",
    "        person_tags\n",
    "        ai_tags\n",
    "        summary\n",
    "        summary_60_words\n",
    "        title\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  '''\n",
    "  offset = 0\n",
    "  mutation_query = \"\"\"\n",
    "  mutation MyMutation($objects: [synopse_articles_t_v5_article_groups_vectors_insert_input!] = {}, $updates: [synopse_articles_t_v3_article_groups_l2_updates!] = {where: {}}) {\n",
    "    insert_synopse_articles_t_v5_article_groups_vectors(objects: $objects, on_conflict: {constraint: t_v5_article_groups_vectors_article_group_id_key}) {\n",
    "      affected_rows\n",
    "    }\n",
    "    update_synopse_articles_t_v3_article_groups_l2_many(updates: $updates) {\n",
    "      affected_rows\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "  while True:\n",
    "    variables = {\n",
    "    \"limit\": 20,\n",
    "    \"offset\": offset\n",
    "    }\n",
    "    synopse_articles_t_v5_article_groups_vectors_insert_input_loc = []\n",
    "    synopse_articles_t_v3_article_groups_l2_updates_loc = []\n",
    "    response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "    if len(response_data['data']['synopse_articles_t_v3_article_groups_l2']) == 0:\n",
    "        break\n",
    "    p1 = []\n",
    "    article_group_ids = []\n",
    "    for response in response_data['data']['synopse_articles_t_v3_article_groups_l2']:\n",
    "      article_group_ids.append( response['t_v4_article_groups_l2_detail']['article_group_id'] )\n",
    "      tags  = ', '.join(response['t_v4_article_groups_l2_detail']['keywords_tags'] ) + \" \" + ', '.join(response['t_v4_article_groups_l2_detail']['location_tags'] ) + \" \" + ', '.join(response['t_v4_article_groups_l2_detail']['org_tags'] ) + \" \" + ', '.join(response['t_v4_article_groups_l2_detail']['person_tags'] ) + \" \" + ', '.join(response['t_v4_article_groups_l2_detail']['ai_tags'] ) + \" \" + ', '.join(response['t_v4_article_groups_l2_detail']['ai_tags'] )\n",
    "      p12 = response['t_v4_article_groups_l2_detail']['title'] + \"\\n\" + response['t_v4_article_groups_l2_detail']['summary'] + \"\\n\" + response['t_v4_article_groups_l2_detail']['summary_60_words'] + tags\n",
    "      p1.append(p12)\n",
    "    embeddings = model.encode(p1)\n",
    "    for i in range(0,len(article_group_ids)):\n",
    "      synopse_articles_t_v5_article_groups_vectors_insert_input_loc.append({\n",
    "          \"article_group_id\": article_group_ids[i],\n",
    "          \"vector1\":  str(embeddings[i].tolist()),\n",
    "          }\n",
    "          )\n",
    "      synopse_articles_t_v3_article_groups_l2_updates_loc.append({\n",
    "          \"where\": {\"id\" : { \"_eq\": article_group_ids[i] }},\n",
    "          \"_set\": {\"is_vectorized\": 1}\n",
    "          })\n",
    "\n",
    "    mutation_variables = {\n",
    "        \"objects\": synopse_articles_t_v5_article_groups_vectors_insert_input_loc,\n",
    "        \"updates\": synopse_articles_t_v3_article_groups_l2_updates_loc,\n",
    "        }\n",
    "    out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "  def trending_search_tags(offset1):\n",
    "    model = SentenceTransformer('BAAI/bge-large-zh-v1.5')\n",
    "    graphql_query = '''\n",
    "    query MyQuery {\n",
    "      synopse_articles_t_v4_tags_hierarchy(where: {tags1: {_is_null: true}}) {\n",
    "        tag\n",
    "        tag_description\n",
    "        id\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    offset = 0\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($updates: [synopse_articles_t_v4_tags_hierarchy_updates!] = {where: {}}) {\n",
    "      update_synopse_articles_t_v4_tags_hierarchy_many(updates: $updates) {\n",
    "        affected_rows\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    variables = {\n",
    "    }\n",
    "    synopse_articles_t_v4_tags_hierarchy_updates_loc = []\n",
    "    response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "    p1 = []\n",
    "    tag_ids = []\n",
    "    for response in response_data['data']['synopse_articles_t_v4_tags_hierarchy']:\n",
    "      tag_ids.append( response['id'] )\n",
    "      p12 = \"Represent this query for searching documents: show articles most relevent to the tag \"  + response['tag'] + \"\\n\" + \"description of the tag for more precise search results is \" + response['tag_description']\n",
    "      p1.append(p12)\n",
    "\n",
    "    embeddings = model.encode(p1)\n",
    "    for i in range(0,len(tag_ids)):\n",
    "      synopse_articles_t_v4_tags_hierarchy_updates_loc.append({\n",
    "          \"where\": {\"id\" : { \"_eq\": tag_ids[i] }},\n",
    "          \"_set\": {\"tags1\":  str(embeddings[i].tolist())}\n",
    "          })\n",
    "\n",
    "    mutation_variables = {\n",
    "        \"updates\": synopse_articles_t_v4_tags_hierarchy_updates_loc,\n",
    "        }\n",
    "    out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "    graphql_query = '''\n",
    "    query MyQuery {\n",
    "      synopse_realtime_t_temp_trending_searches(where: {search1: {_is_null: true}}) {\n",
    "        id\n",
    "        search_text\n",
    "      }\n",
    "    }\n",
    "\n",
    "    '''\n",
    "    offset = 0\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($updates: [synopse_realtime_t_temp_trending_searches_updates!] = {where: {}}) {\n",
    "      update_synopse_realtime_t_temp_trending_searches_many(updates: $updates) {\n",
    "        affected_rows\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    variables = {\n",
    "    }\n",
    "    synopse_realtime_t_temp_trending_searches_updates_loc = []\n",
    "    response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "    p1 = []\n",
    "    search_ids = []\n",
    "    for response in response_data['data']['synopse_realtime_t_temp_trending_searches']:\n",
    "      search_ids.append( response['id'] )\n",
    "      p12 = \"Represent this query for searching documents: \"  + response['search_text']\n",
    "      p1.append(p12)\n",
    "\n",
    "    embeddings = model.encode(p1)\n",
    "    for i in range(0,len(search_ids)):\n",
    "      synopse_realtime_t_temp_trending_searches_updates_loc.append({\n",
    "          \"where\": {\"id\" : { \"_eq\": search_ids[i] }},\n",
    "          \"_set\": {\"search1\":  str(embeddings[i].tolist())}\n",
    "          })\n",
    "\n",
    "    mutation_variables = {\n",
    "        \"updates\": synopse_realtime_t_temp_trending_searches_updates_loc,\n",
    "        }\n",
    "    out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "https://www.cnn.com/business/live-news/stock-market-bank-earnings/index.html Markets digest bank earnings after recent turmoil  na https://cdn.cnn.com/cnnnext/dam/assets/230328155427-01-nyse-0328-super-169.jpg 2023-12-18T07:41:10.317865+00:00 1\n",
      "https://www.cnn.com/2023/04/13/success/tax-filing-tips/index.html Still haven't filed your taxes? Here's what you need to know So far this tax season, the IRS has received more than 90 million income tax returns for 2022. na https://cdn.cnn.com/cnnnext/dam/assets/230411161107-stressed-man-stock-super-169.jpeg 2023-12-18T07:41:10.317865+00:00 1\n",
      "https://www.cnn.com/2023/04/14/economy/march-retail-sales/index.html Retail spending fell in March as consumers pull back Spending at US retailers fell in March as consumers pulled back amid recessionary fears fueled by the banking crisis. na https://cdn.cnn.com/cnnnext/dam/assets/230412211243-grocery-store-california-230412-super-169.jpg 2023-12-18T07:41:10.317865+00:00 1\n",
      "https://www.cnn.com/2023/04/14/media/fox-news-dominion-hnk-intl/index.html Analysis: Fox News is about to enter the true No Spin Zone This is it. na https://cdn.cnn.com/cnnnext/dam/assets/230413121210-01-dominion-courtroom-super-169.jpg 2023-12-18T07:41:10.317865+00:00 1\n",
      "https://www.cnn.com/2023/04/13/business/silicon-valley-bank-entrepreneurs-of-color-reaj/index.html Silicon Valley Bank collapse renews calls to address disparities impacting entrepreneurs of color When customers at Silicon Valley Bank rushed to withdraw billions of dollars last month, venture capitalist Arlan Hamilton stepped in to help some of the founders of color who panicked about losing access to payroll funds. na https://cdn.cnn.com/cnnnext/dam/assets/230410105412-08-silicon-valley-bank-svb-0310-super-169.jpg 2023-12-18T07:41:10.317865+00:00 1\n",
      "https://www.cnn.com/2022/03/21/us/lake-powell-capacity-shrinking-drought-climate/index.html Not only is Lake Powell's water level plummeting because of drought, its total capacity is shrinking, too Lake Powell, the second-largest human-made reservoir in the US, has lost nearly 7% of its potential storage capacity since 1963, when Glen Canyon Dam was built, a new report shows. na https://cdn.cnn.com/cnnnext/dam/assets/220321144911-lake-powell-utah-062421-file-super-169.jpg 2023-12-18T07:41:10.317865+00:00 1\n",
      "https://www.cnn.com/2022/03/22/world/air-pollution-2021-iqair-report-climate/index.html These were the best and worst places for air quality in 2021, new report shows Air pollution spiked to unhealthy levels around the world in 2021, according to a new report. na https://cdn.cnn.com/cnnnext/dam/assets/220321134236-01-india-pollution-2021-super-169.jpg 2023-12-18T07:41:10.317865+00:00 1\n",
      "https://www.cnn.com/2022/03/20/us/solar-power-on-big-box-store-rooftops-climate/index.html Big-box stores could help slash emissions and save millions by putting solar panels on roofs. Why aren't more of them doing it? As the US attempts to wean itself off its heavy reliance on fossil fuels and shift to cleaner energy sources, many experts are eyeing a promising solution: your neighborhood big-box stores and shopping malls. na https://cdn.cnn.com/cnnnext/dam/assets/220309154435-02-big-box-rooftop-solar-climate-super-169.jpg 2023-12-18T07:41:10.317865+00:00 1\n",
      "https://www.cnn.com/style/article/blackpink-coachella-2023-hanboks-lotw/index.html Look of the Week: Blackpink headline Coachella in Korean hanboks Bringing the second day of this year's Coachella to a close, K-Pop girl group Blackpink made history Saturday night when they became the first Asian act to ever headline the festival. To a crowd of, reportedly, over 125,000 people, Jennie, Jisoo, Lisa and Ros used the ground-breaking moment to pay homage to Korean heritage by arriving onstage in hanboks: a traditional type of dress. na https://cdn.cnn.com/cnnnext/dam/assets/230417143301-01-lotw-0417-blackpink-coachella-2023-super-169.jpg 2023-12-18T07:41:10.317865+00:00 1\n",
      "https://www.cnn.com/style/article/old-masters-da-vinci-egg-yolk-painting-scn/index.html Scientists identify secret ingredient in Leonardo da Vinci paintings \"Old Masters\" such as Leonardo da Vinci, Sandro Botticelli and Rembrandt may have used proteins, especially egg yolk, in their oil paintings, according to a new study. na https://cdn.cnn.com/cnnnext/dam/assets/230328171153-01-old-masters-egg-yolk-painting-super-169.jpg 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/style/article/playboy-the-conversation/index.html How Playboy cut ties with Hugh Hefner to create a post-MeToo brand Hugh Hefner launched Playboy Magazine 70 years ago this year. The first issue included a nude photograph of Marilyn Monroe, which he had purchased and published without her knowledge or consent. na https://cdn.cnn.com/cnnnext/dam/assets/230410154711-03-playboy-revamp-hugh-hefner-2008-restricted-super-169.jpg 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/style/article/fashion-week-fall-winter-2023-size-diversity-skinny-wegovy/index.html 'A definitive backslide.' Inside fashion's worrying runway trend Now that the Fall-Winter 2023 catwalks have been disassembled, it's clear one trend was more pervasive than any collective penchant for ruffles, pleated skirts or tailored coats. na https://cdn.cnn.com/cnnnext/dam/assets/230404100406-03-fashion-fall-winter-skinny-fendi-restricted-super-169.jpg 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/style/article/michael-jordan-sneakers-1998-finals-sothebys-auction-record/index.html Michael Jordan's 1998 NBA Finals sneakers sell for a record $2.2 million In 1998, Michael Jordan laced up a pair of his iconic black and red Air Jordan 13s to bring home a Bulls victory during Game 2 of his final NBA championship  and now they are the most expensive sneakers ever to sell at auction.\n",
      "\n",
      "The game-winning sneakers sold for $2.2 million at Sotheby's in New York on Tuesday, smashing the sneaker auction record of $1.47 million, set in 2021 by a pair of Nike Air Ships that Jordan wore earlier in his career. na https://cdn.cnn.com/cnnnext/dam/assets/230314141314-03-michael-jordan-air-1998-sale-super-169.jpg 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/style/article/francois-prost-gentlemens-club/index.html The surreal facades of America's strip clubs Some people travel the world in search of adventure, while others seek out natural wonders, cultural landmarks or culinary experiences. But French photographer Franois Prost was looking for something altogether different during his recent road trip across America: strip clubs. na https://cdn.cnn.com/cnnnext/dam/assets/230219164059-02-francois-prost-gentlemens-club-super-169.jpg 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/travel/article/airplane-mode-reasons-why/index.html Here's the real reason to turn on airplane mode when you fly We all know the routine by heart: \"Please ensure your seats are in the upright position, tray tables stowed, window shades are up, laptops are stored in the overhead bins and electronic devices are set to flight mode.\" na https://cdn.cnn.com/cnnnext/dam/assets/230402145738-airplane-phone-stock-restricted-super-169.jpg 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/travel/article/hippo-attack-avoid-survive-paul-templer/index.html 'I was up to my waist down a hippo's throat.' He survived, and here's his advice Paul Templer was living his best life. na https://cdn.cnn.com/cnnnext/dam/assets/230411150258-01-hippo-attack-top-super-169.jpg 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/travel/article/couple-turned-abandoned-japanese-home-into-guest-house/index.html They bought an abandoned 'ghost house' in the Japanese countryside He'd spent years backpacking around the world, and Japanese traveler Daisuke Kajiyama was finally ready to return home to pursue his long-held dream of opening up a guesthouse. na https://cdn.cnn.com/cnnnext/dam/assets/230330153347-05-body-abandoned-japanese-home-into-guesthouse-super-169.jpg 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/travel/article/mongolia-reasons-to-visit-2023/index.html Relaxed entry rules make it easier than ever to visit this stunning Asian nation Due to its remoteness and short summer season, Mongolia has long been a destination overlooked by travelers. na https://cdn.cnn.com/cnnnext/dam/assets/230328084634-02-mongolia-reasons-to-visit-2023-card-super-169.jpg 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/travel/article/china-beautiful-great-wall-sections-cmd/index.html The most beautiful sections of China's Great Wall Having lived in Beijing for almost 12 years, I've had plenty of time to travel widely in China. na https://cdn.cnn.com/cnnnext/dam/assets/221215113918-01-china-great-wall-sections-yongtai-turtle-city-super-169.jpg 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/specials/travel-newsletter Sign up to our newsletter for a weekly roundup of travel news  na https://cdn.cnn.com/cnnnext/dam/assets/210503123119-unlocking-the-world-stacked-logo-super-169.png 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/2022/12/11/us/cnn-heroes-all-star-tribute-hero-of-the-year/index.html Nelly Cheboi, who creates computer labs for Kenyan schoolchildren, is CNN's Hero of the Year Celebrities and musicians are coming together tonight to honor everyday people making the world a better place. na https://cdn.cnn.com/cnnnext/dam/assets/221211222825-32-cnn-heroes-2022-super-169.jpg 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/specials/cnn-heroes-salutes-special CNN Heroes: Sharing the Spotlight  na https://cdn.cnn.com/cnnnext/dam/assets/221209134855-sharing-the-spotlight-card-cnnheroes-super-169.jpg 2023-12-18T07:41:10.318911+00:00 1\n",
      "https://www.cnn.com/videos/tv/2021/11/26/how-to-donate-matching-cnnheroes.cnn Donate now to a Top 10 CNN Hero Anderson Cooper explains how you can easily donate to any of the 2021 Top 10 CNN Heroes. na https://cdn.cnn.com/cnnnext/dam/assets/211126161149-how-to-donate-matching-cnnheroes-00000000-super-169.png 2023-12-18T07:41:10.319929+00:00 1\n",
      "{'query': '\\n    mutation MyMutation($objects: [synopse_articles_t_v1_rss1_articles_insert_input!] = {}) {\\n        insert_synopse_articles_t_v1_rss1_articles(objects: $objects, on_conflict: {constraint: t_v1_rss1_articals_post_link_key}) {\\n            affected_rows\\n        }\\n    }\\n    ', 'variables': {'objects': [{'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/business/live-news/stock-market-bank-earnings/index.html', 'title': 'Markets digest bank earnings after recent turmoil', 'summary': '', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230328155427-01-nyse-0328-super-169.jpg', 'post_published': '2023-12-18T07:41:10.317865+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/2023/04/13/success/tax-filing-tips/index.html', 'title': \"Still haven't filed your taxes? Here's what you need to know\", 'summary': 'So far this tax season, the IRS has received more than 90 million income tax returns for 2022.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230411161107-stressed-man-stock-super-169.jpeg', 'post_published': '2023-12-18T07:41:10.317865+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/2023/04/14/economy/march-retail-sales/index.html', 'title': 'Retail spending fell in March as consumers pull back', 'summary': 'Spending at US retailers fell in March as consumers pulled back amid recessionary fears fueled by the banking crisis.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230412211243-grocery-store-california-230412-super-169.jpg', 'post_published': '2023-12-18T07:41:10.317865+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/2023/04/14/media/fox-news-dominion-hnk-intl/index.html', 'title': 'Analysis: Fox News is about to enter the true No Spin Zone', 'summary': 'This is it.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230413121210-01-dominion-courtroom-super-169.jpg', 'post_published': '2023-12-18T07:41:10.317865+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/2023/04/13/business/silicon-valley-bank-entrepreneurs-of-color-reaj/index.html', 'title': 'Silicon Valley Bank collapse renews calls to address disparities impacting entrepreneurs of color', 'summary': 'When customers at Silicon Valley Bank rushed to withdraw billions of dollars last month, venture capitalist Arlan Hamilton stepped in to help some of the founders of color who panicked about losing access to payroll funds.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230410105412-08-silicon-valley-bank-svb-0310-super-169.jpg', 'post_published': '2023-12-18T07:41:10.317865+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/2022/03/21/us/lake-powell-capacity-shrinking-drought-climate/index.html', 'title': \"Not only is Lake Powell's water level plummeting because of drought, its total capacity is shrinking, too\", 'summary': 'Lake Powell, the second-largest human-made reservoir in the US, has lost nearly 7% of its potential storage capacity since 1963, when Glen Canyon Dam was built, a new report shows.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/220321144911-lake-powell-utah-062421-file-super-169.jpg', 'post_published': '2023-12-18T07:41:10.317865+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/2022/03/22/world/air-pollution-2021-iqair-report-climate/index.html', 'title': 'These were the best and worst places for air quality in 2021, new report shows', 'summary': 'Air pollution spiked to unhealthy levels around the world in 2021, according to a new report.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/220321134236-01-india-pollution-2021-super-169.jpg', 'post_published': '2023-12-18T07:41:10.317865+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/2022/03/20/us/solar-power-on-big-box-store-rooftops-climate/index.html', 'title': \"Big-box stores could help slash emissions and save millions by putting solar panels on roofs. Why aren't more of them doing it?\", 'summary': 'As the US attempts to wean itself off its heavy reliance on fossil fuels and shift to cleaner energy sources, many experts are eyeing a promising solution: your neighborhood big-box stores and shopping malls.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/220309154435-02-big-box-rooftop-solar-climate-super-169.jpg', 'post_published': '2023-12-18T07:41:10.317865+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/style/article/blackpink-coachella-2023-hanboks-lotw/index.html', 'title': 'Look of the Week: Blackpink headline Coachella in Korean hanboks', 'summary': \"Bringing the second day of this year's Coachella to a close, K-Pop girl group Blackpink made history Saturday night when they became the first Asian act to ever headline the festival. To a crowd of, reportedly, over 125,000 people, Jennie, Jisoo, Lisa and Ros used the ground-breaking moment to pay homage to Korean heritage by arriving onstage in hanboks: a traditional type of dress.\", 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230417143301-01-lotw-0417-blackpink-coachella-2023-super-169.jpg', 'post_published': '2023-12-18T07:41:10.317865+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/style/article/old-masters-da-vinci-egg-yolk-painting-scn/index.html', 'title': 'Scientists identify secret ingredient in Leonardo da Vinci paintings', 'summary': '\"Old Masters\" such as Leonardo da Vinci, Sandro Botticelli and Rembrandt may have used proteins, especially egg yolk, in their oil paintings, according to a new study.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230328171153-01-old-masters-egg-yolk-painting-super-169.jpg', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/style/article/playboy-the-conversation/index.html', 'title': 'How Playboy cut ties with Hugh Hefner to create a post-MeToo brand', 'summary': 'Hugh Hefner launched Playboy Magazine 70 years ago this year. The first issue included a nude photograph of Marilyn Monroe, which he had purchased and published without her knowledge or consent.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230410154711-03-playboy-revamp-hugh-hefner-2008-restricted-super-169.jpg', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/style/article/fashion-week-fall-winter-2023-size-diversity-skinny-wegovy/index.html', 'title': \"'A definitive backslide.' Inside fashion's worrying runway trend\", 'summary': \"Now that the Fall-Winter 2023 catwalks have been disassembled, it's clear one trend was more pervasive than any collective penchant for ruffles, pleated skirts or tailored coats.\", 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230404100406-03-fashion-fall-winter-skinny-fendi-restricted-super-169.jpg', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/style/article/michael-jordan-sneakers-1998-finals-sothebys-auction-record/index.html', 'title': \"Michael Jordan's 1998 NBA Finals sneakers sell for a record $2.2 million\", 'summary': \"In 1998, Michael Jordan laced up a pair of his iconic black and red Air Jordan 13s to bring home a Bulls victory during Game 2 of his final NBA championship  and now they are the most expensive sneakers ever to sell at auction.\\n\\nThe game-winning sneakers sold for $2.2 million at Sotheby's in New York on Tuesday, smashing the sneaker auction record of $1.47 million, set in 2021 by a pair of Nike Air Ships that Jordan wore earlier in his career.\", 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230314141314-03-michael-jordan-air-1998-sale-super-169.jpg', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/style/article/francois-prost-gentlemens-club/index.html', 'title': \"The surreal facades of America's strip clubs\", 'summary': 'Some people travel the world in search of adventure, while others seek out natural wonders, cultural landmarks or culinary experiences. But French photographer Franois Prost was looking for something altogether different during his recent road trip across America: strip clubs.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230219164059-02-francois-prost-gentlemens-club-super-169.jpg', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/travel/article/airplane-mode-reasons-why/index.html', 'title': \"Here's the real reason to turn on airplane mode when you fly\", 'summary': 'We all know the routine by heart: \"Please ensure your seats are in the upright position, tray tables stowed, window shades are up, laptops are stored in the overhead bins and electronic devices are set to flight mode.\"', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230402145738-airplane-phone-stock-restricted-super-169.jpg', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/travel/article/hippo-attack-avoid-survive-paul-templer/index.html', 'title': \"'I was up to my waist down a hippo's throat.' He survived, and here's his advice\", 'summary': 'Paul Templer was living his best life.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230411150258-01-hippo-attack-top-super-169.jpg', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/travel/article/couple-turned-abandoned-japanese-home-into-guest-house/index.html', 'title': \"They bought an abandoned 'ghost house' in the Japanese countryside\", 'summary': \"He'd spent years backpacking around the world, and Japanese traveler Daisuke Kajiyama was finally ready to return home to pursue his long-held dream of opening up a guesthouse.\", 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230330153347-05-body-abandoned-japanese-home-into-guesthouse-super-169.jpg', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/travel/article/mongolia-reasons-to-visit-2023/index.html', 'title': 'Relaxed entry rules make it easier than ever to visit this stunning Asian nation', 'summary': 'Due to its remoteness and short summer season, Mongolia has long been a destination overlooked by travelers.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/230328084634-02-mongolia-reasons-to-visit-2023-card-super-169.jpg', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/travel/article/china-beautiful-great-wall-sections-cmd/index.html', 'title': \"The most beautiful sections of China's Great Wall\", 'summary': \"Having lived in Beijing for almost 12 years, I've had plenty of time to travel widely in China.\", 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/221215113918-01-china-great-wall-sections-yongtai-turtle-city-super-169.jpg', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/specials/travel-newsletter', 'title': 'Sign up to our newsletter for a weekly roundup of travel news', 'summary': '', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/210503123119-unlocking-the-world-stacked-logo-super-169.png', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/2022/12/11/us/cnn-heroes-all-star-tribute-hero-of-the-year/index.html', 'title': \"Nelly Cheboi, who creates computer labs for Kenyan schoolchildren, is CNN's Hero of the Year\", 'summary': 'Celebrities and musicians are coming together tonight to honor everyday people making the world a better place.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/221211222825-32-cnn-heroes-2022-super-169.jpg', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/specials/cnn-heroes-salutes-special', 'title': 'CNN Heroes: Sharing the Spotlight', 'summary': '', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/221209134855-sharing-the-spotlight-card-cnnheroes-super-169.jpg', 'post_published': '2023-12-18T07:41:10.318911+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}, {'rss1_link': 'http://rss.cnn.com/rss/edition_world.rss', 'post_link': 'https://www.cnn.com/videos/tv/2021/11/26/how-to-donate-matching-cnnheroes.cnn', 'title': 'Donate now to a Top 10 CNN Hero', 'summary': 'Anderson Cooper explains how you can easily donate to any of the 2021 Top 10 CNN Heroes.', 'author': 'na', 'image_link': 'https://cdn.cnn.com/cnnnext/dam/assets/211126161149-how-to-donate-matching-cnnheroes-00000000-super-169.png', 'post_published': '2023-12-18T07:41:10.319929+00:00', 'is_default_image': 1, 'tags': ['world', 'cnn']}]}}\n",
      "{'data': {'insert_synopse_articles_t_v1_rss1_articles': {'affected_rows': 0}}}\n",
      "(True, {'data': {'insert_synopse_articles_t_v1_rss1_articles': {'affected_rows': 0}}})\n"
     ]
    }
   ],
   "source": [
    "def update_articles():\n",
    "    graphql_query = '''\n",
    "    query MyQuery($link_type: Int!) {\n",
    "        synopse_articles_t_v1_rss1_feed_links(where: {rss1_link_type: {_eq: $link_type}}) {\n",
    "          rss1_link\n",
    "          rss1_link_name\n",
    "          outlet\n",
    "        }\n",
    "      }\n",
    "    '''\n",
    "    # Define the variables dictionary\n",
    "    variables = {\n",
    "        \"link_type\": 11\n",
    "    }\n",
    "    rss1_links_array = []\n",
    "    rss1_link_name = []\n",
    "    outlet = []\n",
    "    response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "    if response_data:\n",
    "        rss1_links_array = [item[\"rss1_link\"] for item in response_data[\"data\"][\"synopse_articles_t_v1_rss1_feed_links\"]]\n",
    "        outlet = [item[\"outlet\"] for item in response_data[\"data\"][\"synopse_articles_t_v1_rss1_feed_links\"]]\n",
    "        rss1_link_name = [item[\"rss1_link_name\"] for item in response_data[\"data\"][\"synopse_articles_t_v1_rss1_feed_links\"]]\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [synopse_articles_t_v1_rss1_articles_insert_input!] = {}) {\n",
    "        insert_synopse_articles_t_v1_rss1_articles(objects: $objects, on_conflict: {constraint: t_v1_rss1_articals_post_link_key}) {\n",
    "            affected_rows\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    for i in range(0,len(rss1_links_array)):\n",
    "        NewsFeed = feedparser.parse(rss1_links_array[i])\n",
    "        articles = []\n",
    "        if (rss1_links_array[i] == \"http://rss.cnn.com/rss/edition_world.rss\"):\n",
    "            print(\"############################################################\")\n",
    "            for entry in NewsFeed.entries:\n",
    "                if outlet[i] +\".\" in entry.link:\n",
    "                    is_default_image = 0\n",
    "                    title = entry.title\n",
    "                    summary = ''\n",
    "                    if 'summary' in entry:\n",
    "                        summary_nofil = entry.summary\n",
    "                        summary = re.sub('<[^<]+?>', '', summary_nofil)\n",
    "                    image_url = \"\"\n",
    "                    if 'media_content' in entry:\n",
    "                        image_url = entry['media_content'][0]['url']\n",
    "                        is_default_image = 1\n",
    "                    if 'links' in entry:\n",
    "                        for link in entry.links:\n",
    "                            if link.type == \"image/jpeg\":\n",
    "                                image_url= link.href\n",
    "                                is_default_image = 1\n",
    "                                break\n",
    "                    post_link = entry.link\n",
    "                    published = datetime.now(timezone.utc).isoformat()\n",
    "                    if 'published' in entry:\n",
    "                        published = entry.published\n",
    "                    datevalidation = is_valid_timezone_format(published)\n",
    "                    if datevalidation[0]:\n",
    "                        hasura_timestamp = datevalidation[1]\n",
    "                    if check_date_format(published):\n",
    "                        hasura_timestamp = published\n",
    "                    else:\n",
    "                        hasura_timestamp = datetime.now().astimezone(timezone.utc).isoformat()\n",
    "                    if \"author\" in entry:\n",
    "                        author = entry.author\n",
    "                    else:\n",
    "                        author = \"na\"\n",
    "                    tags = []\n",
    "                    tags.append(rss1_link_name[i])\n",
    "                    tags.append(outlet[i])\n",
    "                    if 'tags' in entry:\n",
    "                        for tag in entry.tags:\n",
    "                            tags.append(tag.term)\n",
    "                    if outlet[i] in post_link:\n",
    "                        articles.append({\n",
    "                                \"rss1_link\": rss1_links_array[i],\n",
    "                                \"post_link\": post_link,\n",
    "                                \"title\": title,\n",
    "                                \"summary\": summary,\n",
    "                                \"author\": author,\n",
    "                                \"image_link\" : image_url,\n",
    "                                \"post_published\": hasura_timestamp,\n",
    "                                \"is_default_image\": is_default_image,\n",
    "                                \"tags\": tags,\n",
    "                            }\n",
    "                        )\n",
    "                    print(post_link, title, summary, author, image_url, hasura_timestamp, is_default_image)\n",
    "            mutation_variables = {\n",
    "                \"objects\": articles\n",
    "            }\n",
    "            print({'query': mutation_query, 'variables': mutation_variables})\n",
    "            out1 = mutation_hasura_graphql(endpoint = endpoint, admin_key = admin_key, mutation_query = mutation_query, mutation_variables = mutation_variables)\n",
    "            print(out1)\n",
    "update_articles()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syn1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
