{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import feedparser\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "import pyperclip\n",
    "import time\n",
    "from transformers import pipeline\n",
    "import numpy\n",
    "import torch\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import json \n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "import tiktoken\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "from time import monotonic\n",
    "\n",
    "endpoint = \"https://enabling-elk-81.hasura.app/v1/graphql\"\n",
    "admin_key = \"bdAHRgu0lLGgF38TkQ0eL3ynNGLC23jxB4tnFzMiiSFh94YVMMHiIIouK4YfnEoB\"\n",
    "\n",
    "def query_hasura_graphql(endpoint, admin_key, query, variables):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'x-hasura-admin-secret': f'{admin_key}'\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'query': query,\n",
    "        'variables': variables\n",
    "    }\n",
    "    response = requests.post(endpoint, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def is_valid_timezone_format(published):\n",
    "    try:\n",
    "        # Attempt to parse the string\n",
    "        date_format = \"%a, %d %b %Y %H:%M:%S %z\"\n",
    "        date_object = datetime.strptime(published, date_format)\n",
    "        \n",
    "        hasura_timestamp = date_object.astimezone(timezone.utc).isoformat()\n",
    "        return True, hasura_timestamp\n",
    "    except ValueError:\n",
    "        # If parsing fails, the string is not in the correct format\n",
    "        return False, None\n",
    "\n",
    "def check_date_format(date_string):\n",
    "    try:\n",
    "        datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%S%z')\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "        \n",
    "def mutation_hasura_graphql(endpoint, admin_key, mutation_query, mutation_variables):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'x-hasura-admin-secret': f'{admin_key}'\n",
    "    }\n",
    "    response = requests.post(endpoint, json={'query': mutation_query, 'variables': mutation_variables}, headers=headers)\n",
    "    if response.ok:\n",
    "        data = response.json()\n",
    "        print(data)\n",
    "        return True, data\n",
    "    else:\n",
    "        print(f\"Mutation failed with status code {response.status_code}: {response.text}\")\n",
    "        return False, None\n",
    "\n",
    "def update_articles():\n",
    "    graphql_query = '''\n",
    "    query MyQuery($link_type: Int!) {\n",
    "        articles_t_v1_rss1_feed_links(where: {rss1_link_type: {_eq: $link_type}}) {\n",
    "            rss1_link\n",
    "            outlet\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    # Define the variables dictionary\n",
    "    variables = {\n",
    "        \"link_type\": 11\n",
    "    }\n",
    "    rss1_links_array = []\n",
    "    outlet = []\n",
    "    response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "    if response_data:\n",
    "        rss1_links_array = [item[\"rss1_link\"] for item in response_data[\"data\"][\"articles_t_v1_rss1_feed_links\"]]\n",
    "        outlet = [item[\"outlet\"] for item in response_data[\"data\"][\"articles_t_v1_rss1_feed_links\"]]\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_T_v1_rss1_articals_insert_input!] = {}) {\n",
    "        insert_articles_T_v1_rss1_articals(objects: $objects, on_conflict: {constraint: T_v1_rss1_articals_post_link_key}) {\n",
    "            affected_rows\n",
    "            returning {\n",
    "            id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0,len(rss1_links_array)-1):\n",
    "        NewsFeed = feedparser.parse(rss1_links_array[i])\n",
    "        print(\"############################################################\")\n",
    "        print(rss1_links_array[i])\n",
    "        articles = []\n",
    "        for entry in NewsFeed.entries:\n",
    "            # print(entry.link)\n",
    "            is_default_image = 0\n",
    "            title = entry.title\n",
    "            summary = ''\n",
    "            if 'summary' in entry:\n",
    "                summary_nofil = entry.summary\n",
    "                summary = re.sub('<[^<]+?>', '', summary_nofil)\n",
    "            image_url = \"\"\n",
    "            if 'media_content' in entry:\n",
    "                image_url = entry['media_content'][0]['url']\n",
    "                is_default_image = 1\n",
    "            if 'links' in entry:\n",
    "                for link in entry.links:\n",
    "                    if link.type == \"image/jpeg\":\n",
    "                        image_url= link.href\n",
    "                        is_default_image = 1\n",
    "                        break\n",
    "            post_link = entry.link\n",
    "            published = datetime.now(timezone.utc).isoformat()\n",
    "            if 'published' in entry:\n",
    "                published = entry.published\n",
    "            datevalidation = is_valid_timezone_format(published)\n",
    "            if datevalidation[0]:\n",
    "                hasura_timestamp = datevalidation[1]\n",
    "            if check_date_format(published):\n",
    "                hasura_timestamp = published\n",
    "            else:\n",
    "                hasura_timestamp = datetime.now().astimezone(timezone.utc).isoformat()\n",
    "            if \"author\" in entry:\n",
    "                author = entry.author\n",
    "            else:\n",
    "                author = \"na\"\n",
    "            if outlet[i] in post_link:\n",
    "                articles.append({\n",
    "                        \"rss1_link\": rss1_links_array[i],\n",
    "                        \"post_link\": post_link,\n",
    "                        \"title\": title,\n",
    "                        \"summary\": summary,\n",
    "                        \"author\": author,\n",
    "                        \"image_link\" : image_url,\n",
    "                        \"post_published\": hasura_timestamp,\n",
    "                        \"is_default_image\": is_default_image,\n",
    "                    }\n",
    "                )\n",
    "            #print(feed_link, post_link, title, summary, author, image_url, hasura_timestamp, is_default_image)\n",
    "        mutation_variables = {\n",
    "            \"objects\": articles\n",
    "        }\n",
    "        #print({'query': mutation_query, 'variables': mutation_variables})\n",
    "        out1 = mutation_hasura_graphql(endpoint = endpoint, admin_key = admin_key, mutation_query = mutation_query, mutation_variables = mutation_variables)\n",
    "\n",
    "def update_article_details(offset1):\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    articles_T_v1_rss1_articals(limit: $limit, offset: $offset, where: {is_in_detail: {_eq: 0}}, order_by: {post_published: desc}) {\n",
    "        post_link\n",
    "        is_default_image\n",
    "        image_link\n",
    "        id\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_T_v1_rss1_articles_detail_insert_input!] = {}, $updates: [articles_T_v1_rss1_articals_updates!] = {where: {}}) {\n",
    "        insert_articles_T_v1_rss1_articles_detail(objects: $objects, on_conflict: {constraint: T_v1_rss1_articles_detail_article_id_key}) {\n",
    "            affected_rows\n",
    "        }\n",
    "        update_articles_T_v1_rss1_articals_many(updates: $updates) {\n",
    "            affected_rows\n",
    "        }\n",
    "        }\n",
    "\n",
    "    \"\"\"    \n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.use_chromium = True\n",
    "    options.page_load_strategy = 'eager'\n",
    "    options.add_argument('--enable-immersive-reader')\n",
    "    driver = webdriver.Edge(options=options)\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 2,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        #print(variables, response_data)\n",
    "        #print(response_data)\n",
    "        post_links_array = []\n",
    "        ids=[]\n",
    "        if response_data:\n",
    "            post_links_array = [item[\"post_link\"] for item in response_data[\"data\"][\"articles_T_v1_rss1_articals\"]]\n",
    "            is_default_image_array = [item[\"is_default_image\"] for item in response_data[\"data\"][\"articles_T_v1_rss1_articals\"]]\n",
    "            image_link_array = [item[\"image_link\"] for item in response_data[\"data\"][\"articles_T_v1_rss1_articals\"]]\n",
    "            ids=[item[\"id\"] for item in response_data[\"data\"][\"articles_T_v1_rss1_articals\"]]\n",
    "        articles_detail = []\n",
    "        articles_update = []\n",
    "        if len(post_links_array) == 0:\n",
    "            break\n",
    "        try:\n",
    "            for a in range(len(post_links_array)):\n",
    "                main_link = post_links_array[a]\n",
    "                print(main_link)\n",
    "                driver.get(main_link)\n",
    "                get_url = driver.current_url\n",
    "                read_link= \"read://\"+get_url\n",
    "                driver.get(read_link)\n",
    "                time.sleep(5)\n",
    "                ActionChains(driver).key_down(Keys.CONTROL).send_keys('a').key_up(Keys.CONTROL).perform()\n",
    "                ActionChains(driver).key_down(Keys.CONTROL).send_keys('c').key_up(Keys.CONTROL).perform()\n",
    "                text = pyperclip.paste()\n",
    "                text2 = text\n",
    "                text3 = text2.split('\\n')\n",
    "                text3 = [s.replace('\\r', '') for s in text3]\n",
    "                special_chars = set(\"!@#$%^&*()_+[]{}|;:'\\\",<>?\")\n",
    "                text4 = [s for s in text3 if len(s) > 0 and (s[0] not in special_chars or s[-1] not in special_chars)]\n",
    "                my_list = text4\n",
    "                if my_list[0] == \"Hmmm… can't reach this page\":\n",
    "                    offset = offset + 1\n",
    "                    break\n",
    "                my_set = set()\n",
    "                desription = []\n",
    "                for item in my_list:\n",
    "                    if item not in my_set:\n",
    "                        desription.append(item)\n",
    "                        my_set.add(item)\n",
    "                #print(desription)\n",
    "                images_final = []\n",
    "                articles_detail.append({\n",
    "                    \"article_id\": ids[a],\n",
    "                    \"title\": desription[0],\n",
    "                    \"description\": desription[1:],\n",
    "                    \"image_link\": images_final,\n",
    "                }\n",
    "                )\n",
    "                if (is_default_image_array[a] == 0 and len(images_final) > 0):\n",
    "                    articles_update.append({\n",
    "                        \"where\": {\"post_link\" : { \"_eq\": main_link }},\n",
    "                        \"_set\": {\"is_in_detail\": 1 , \"image_link\": images_final[0], \"is_default_image\": 1}\n",
    "                    })\n",
    "                else:\n",
    "                    articles_update.append({\n",
    "                        \"where\": {\"post_link\" : { \"_eq\": main_link }},\n",
    "                        \"_set\": {\"is_in_detail\": 1}\n",
    "                    })\n",
    "                \n",
    "                #print(main_link, desription[0], desription[1:], images_final)\n",
    "            #print(articles_update)\n",
    "            mutation_variables = {\n",
    "            \"objects\": articles_detail,\n",
    "            \"updates\": articles_update,\n",
    "            }\n",
    "            out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "        except:\n",
    "            offset = offset + 1\n",
    "            mutation_variables = {\n",
    "            \"objects\": articles_detail,\n",
    "            \"updates\": articles_update,\n",
    "            }\n",
    "            out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "        \n",
    "    driver.quit() \n",
    "\n",
    "def summerizer(offset1): \n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    articles_T_v1_rss1_articles_detail(limit: $limit, offset: $offset, where: {summary: {_is_null: true}}) {\n",
    "        title\n",
    "        description\n",
    "        article_id\n",
    "        T_v1_rss1_artical {\n",
    "        title\n",
    "        summary\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($updates: [articles_T_v1_rss1_articles_detail_updates!] = {where: {}}) {\n",
    "    update_articles_T_v1_rss1_articles_detail_many(updates: $updates) {\n",
    "        affected_rows\n",
    "        returning {\n",
    "        id\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 2,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        rss1_articles_detail_updates = []\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_T_v1_rss1_articles_detail']) == 0:\n",
    "            break\n",
    "        for response in response_data['data']['articles_T_v1_rss1_articles_detail']:\n",
    "            print(response['title'])\n",
    "            article=\"\"\n",
    "            article = article + response['title'] + \" \" +  response['T_v1_rss1_artical']['title'] + \" \" +  response['T_v1_rss1_artical']['summary'] + ', '.join(response['description'])\n",
    "            chunks=[]\n",
    "            max_length = 0\n",
    "            min_length = 0\n",
    "            if len(article) < 1000:\n",
    "                max_length = 150\n",
    "                min_length = 100\n",
    "                chunks.append(article)\n",
    "            elif len(article) < 3000:\n",
    "                max_length = 300\n",
    "                min_length = 200\n",
    "                chunks.append(article)\n",
    "            elif len(article) < 4000:\n",
    "                max_length = 400\n",
    "                min_length = 250\n",
    "                chunks.append(article)\n",
    "            elif len(article) < 8000:\n",
    "                max_length = 200\n",
    "                min_length = 150\n",
    "                midpoint = len(article) // 2\n",
    "                chunks.append(article[:midpoint])\n",
    "                chunks.append(article[midpoint:])\n",
    "            else:\n",
    "                article=article[:8000]\n",
    "                max_length = 200\n",
    "                min_length = 150\n",
    "                midpoint = len(article) // 2\n",
    "                chunks.append(article[:midpoint])\n",
    "                chunks.append(article[midpoint:])\n",
    "\n",
    "            summerize=\"\"\n",
    "            for chunk in chunks:\n",
    "                summerize=summerize + summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']+ \" \"\n",
    "            if len(summerize) > 0:\n",
    "                rss1_articles_detail_updates.append({\n",
    "                    \"where\": {\"article_id\" : { \"_eq\": response['article_id'] }},\n",
    "                    \"_set\": {\"summary\": summerize }\n",
    "                })\n",
    "        mutation_variables = {\n",
    "            \"updates\": rss1_articles_detail_updates,\n",
    "            }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def vectorize(offset1):\n",
    "    model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    articles_T_v1_rss1_articals(limit: $limit, offset: $offset, where: {is_vectorized: {_eq: 0}, is_in_detail: {_eq: 1}}) {\n",
    "        id\n",
    "        title\n",
    "        summary\n",
    "        T_v1_rss1_articles_detail {\n",
    "        summary\n",
    "        tags\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_t_v1_rss1_article_vectors_insert_input!] = {}, $updates: [articles_T_v1_rss1_articals_updates!] = {where: {}}) {\n",
    "    insert_articles_t_v1_rss1_article_vectors(objects: $objects, on_conflict: {constraint: t_v1_rss1_article_vectors_article_id_key}) {\n",
    "        affected_rows\n",
    "        returning {\n",
    "        article_id\n",
    "        }\n",
    "    }\n",
    "    update_articles_T_v1_rss1_articals_many(updates: $updates) {\n",
    "        affected_rows\n",
    "        returning {\n",
    "        id\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        articles_vector1_insert_input_loc=[]\n",
    "        rss1_articals_updates_loc=[]\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_T_v1_rss1_articals']) == 0:\n",
    "            break\n",
    "        #print(json.dumps(response_data, indent=4))\n",
    "        s1= []\n",
    "        ids=[]\n",
    "        for response in response_data['data']['articles_T_v1_rss1_articals']:\n",
    "            article=\"\"\n",
    "            tags=\"\"\n",
    "            if (response['T_v1_rss1_articles_detail']['tags']) is None:\n",
    "                tags = \" \"\n",
    "            else:\n",
    "                tags = \", \".join(response['T_v1_rss1_articles_detail']['tags'])\n",
    "            article = article + response['title'] + \" \" +  response['summary'] + \" \" +  response['T_v1_rss1_articles_detail']['summary'] + tags\n",
    "            s1.append([['Represent the news article for custering and retrieval:  ', article]])\n",
    "            ids.append(response['id'])\n",
    "        embeddings = []\n",
    "        for s in s1:\n",
    "            list_embeddings = numpy.ravel(model.encode(s)).tolist()\n",
    "            embeddings.append(list_embeddings)\n",
    "        for i in range(0,len(ids)):\n",
    "            articles_vector1_insert_input_loc.append({\n",
    "                \"article_id\": ids[i],\n",
    "                \"vector1\": str(embeddings[i]),\n",
    "                }\n",
    "                )\n",
    "            rss1_articals_updates_loc.append({\n",
    "                \"where\": {\"id\" : { \"_eq\": ids[i] }},\n",
    "                \"_set\": {\"is_vectorized\": 1}\n",
    "                })\n",
    "\n",
    "        mutation_variables = {\n",
    "        \"objects\": articles_vector1_insert_input_loc,\n",
    "        \"updates\": rss1_articals_updates_loc,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def grouping(offset1):\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "        articles_T_v1_rss1_articals(limit: $limit, offset: $offset, where: {is_vectorized: {_eq: 1}, is_in_detail: {_eq: 1}, is_grouped: {_eq: 0}}) {\n",
    "            id\n",
    "            t_v1_rss1_article_vector {\n",
    "            vector1\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_t_v1_articles_groups_insert_input!] = {}, $updates: [articles_T_v1_rss1_articals_updates!] = {where: {}}) {\n",
    "        insert_articles_t_v1_articles_groups(objects: $objects, on_conflict: {constraint: t_v1_articles_groups_article_id_key}) {\n",
    "            affected_rows\n",
    "            returning {\n",
    "            article_id\n",
    "            }\n",
    "        }\n",
    "        update_articles_T_v1_rss1_articals_many(updates: $updates) {\n",
    "            affected_rows\n",
    "            returning {\n",
    "            id\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    \"\"\"\n",
    "    func_query = '''\n",
    "    query MyQuery($p_article_id: bigint!) {\n",
    "        articles_get_similar_articles_group(args: {p_article_id: $p_article_id}) {\n",
    "            article_id\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 20,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        articles_groups_insert_input_loc=[]\n",
    "        rss1_articals_updates_loc=[]\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_T_v1_rss1_articals']) == 0:\n",
    "            break\n",
    "        #print(json.dumps(response_data, indent=4))\n",
    "        s1= []\n",
    "        ids=[]\n",
    "        for response in response_data['data']['articles_T_v1_rss1_articals']:\n",
    "            func_variables = {\n",
    "                \"p_article_id\": response['id']\n",
    "                }\n",
    "            func_response_data = query_hasura_graphql(endpoint, admin_key, func_query, func_variables)\n",
    "            article_group = []\n",
    "            #print(json.dumps(func_response_data, indent=4))\n",
    "            if len(func_response_data['data']['articles_get_similar_articles_group']) > 0:\n",
    "                for func_response in func_response_data['data']['articles_get_similar_articles_group']:\n",
    "                    article_group.append(func_response['article_id'])\n",
    "            \n",
    "            articles_groups_insert_input_loc.append({\n",
    "                \"article_id\": response['id'],\n",
    "                \"initial_group\": article_group,\n",
    "                }\n",
    "                )\n",
    "            rss1_articals_updates_loc.append({\n",
    "                \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                \"_set\": {\"is_grouped\": 1}\n",
    "                })\n",
    "\n",
    "        mutation_variables = {\n",
    "        \"objects\": articles_groups_insert_input_loc,\n",
    "        \"updates\": rss1_articals_updates_loc,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def grouping_l1(offset1):\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "          articles_T_v1_rss1_articals(where: {is_grouped: {_eq: 1}}, limit: $limit, offset: $offset) {\n",
    "            id\n",
    "            t_v1_articles_group {\n",
    "            initial_group\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "        mutation MyMutation($objects: [articles_t_v1_articals_groups_l1_insert_input!] = {}, $updates: [articles_T_v1_rss1_articals_updates!] = {where: {}}, $updates1: [articles_t_v1_articals_groups_l1_updates!] = {where: {}}) {\n",
    "        insert_articles_t_v1_articals_groups_l1(objects: $objects) {\n",
    "            affected_rows\n",
    "        }\n",
    "        update_articles_T_v1_rss1_articals_many(updates: $updates) {\n",
    "            affected_rows\n",
    "        }\n",
    "        update_articles_t_v1_articals_groups_l1_many(updates: $updates1) {\n",
    "            affected_rows\n",
    "        }\n",
    "        }\n",
    "    \"\"\"\n",
    "    query2 = '''\n",
    "    query MyQuery($articleid: [bigint!] = [20]) {\n",
    "        articles_t_v1_articles_groups(where: {initial_group: {_contains: $articleid}}) {\n",
    "            article_id\n",
    "            initial_group\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    query3 = '''\n",
    "    query MyQuery($articleid: [bigint!] = [20]) {\n",
    "        articles_t_v1_articals_groups_l1(where: {articles_group: {_contains: $articleid}}) {\n",
    "            articles_group\n",
    "            id\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        articles_grouped_l1_insert_input_loc=[]\n",
    "        rss1_articals_updates_loc=[]\n",
    "        articles_grouped_l1_updates=[]\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_T_v1_rss1_articals']) == 0:\n",
    "            break\n",
    "        #print(json.dumps(response_data, indent=4))\n",
    "        for response in response_data['data']['articles_T_v1_rss1_articals']:\n",
    "\n",
    "            variables2 = {\n",
    "                \"articleid\": [response['id']]\n",
    "                }\n",
    "            func_response_data = query_hasura_graphql(endpoint, admin_key, query2, variables2)\n",
    "            articles_ids = []\n",
    "            print(response['id'])\n",
    "            if len(func_response_data['data']['articles_t_v1_articles_groups']) > 0:\n",
    "                for func_response in func_response_data['data']['articles_t_v1_articles_groups']:\n",
    "                    articles_ids.append(func_response['initial_group'])\n",
    "            \n",
    "            func_response_data1 = query_hasura_graphql(endpoint, admin_key, query3, variables2)\n",
    "            \n",
    "            \n",
    "            if (len(func_response_data1['data']['articles_t_v1_articals_groups_l1']) == 0):\n",
    "                new_lst = []\n",
    "                for sublist in articles_ids:\n",
    "                    for element in sublist:\n",
    "                        new_lst.append(element)\n",
    "                my_list = list(set(new_lst))\n",
    "                print(my_list)\n",
    "                articles_grouped_l1_insert_input_loc.append({\n",
    "                    \"articles_group\": my_list,\n",
    "                    'articles_in_group': len(my_list)\n",
    "                    }\n",
    "                    )\n",
    "                rss1_articals_updates_loc.append({\n",
    "                    \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                    \"_set\": {\"is_grouped\": 2}\n",
    "                    })\n",
    "            else:\n",
    "                articles_ids.append(func_response_data1['data']['articles_t_v1_articals_groups_l1'][0]['articles_group'])\n",
    "                new_lst = []\n",
    "                for sublist in articles_ids:\n",
    "                    for element in sublist:\n",
    "                        new_lst.append(element)\n",
    "                my_list = list(set(new_lst))\n",
    "                articles_grouped_l1_updates.append({\n",
    "                    \"where\": {\"id\" : { \"_eq\": func_response_data1['data']['articles_t_v1_articals_groups_l1'][0]['id'] }},\n",
    "                    \"_set\": {\"articles_group\": my_list, 'articles_in_group': len(my_list)}\n",
    "                    })\n",
    "                rss1_articals_updates_loc.append({\n",
    "                    \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                    \"_set\": {\"is_grouped\": 2}\n",
    "                    })\n",
    "                print(my_list)     \n",
    "        \n",
    "        mutation_variables = {\n",
    "        \"objects\": articles_grouped_l1_insert_input_loc,\n",
    "        \"updates\": rss1_articals_updates_loc,\n",
    "        \"updates1\": articles_grouped_l1_updates,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:    \n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def gen_article():\n",
    "    gpt_35_turbo_max_tokens = 4097\n",
    "    verbose = True\n",
    "    prompt_template = \"\"\"Write a unbiased professional news article for:\n",
    "\n",
    "\n",
    "        {text}\n",
    "\n",
    "\n",
    "        CONSCISE UNBIASED detailed news article with at least 500 words:\"\"\"\n",
    "    OPENAI_API_KEY= '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'\n",
    "    model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model_name=model_name)\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "        articles_t_v1_articals_groups_l1(where: {requires_update: {_eq: 0}, articles_in_group: {_gt: 1}}, limit: $limit, offset: $offset) {\n",
    "            articles_group\n",
    "            id\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    graphql_grquery_article = '''\n",
    "    query MyQuery($article_id: bigint!) {\n",
    "    articles_T_v1_rss1_articals(where: {id: {_eq: $article_id}}) {\n",
    "        title\n",
    "        summary\n",
    "        T_v1_rss1_articles_detail {\n",
    "        summary\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    offset = 1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_t_v1_articals_groups_l1_detail_insert_input!] = {}, $updates: [articles_t_v1_articals_groups_l1_updates!] = {where: {}}) {\n",
    "        insert_articles_t_v1_articals_groups_l1_detail(objects: $objects, on_conflict: {constraint: t_v1_articals_groups_l1_detail_article_group_id_key}) {\n",
    "            affected_rows\n",
    "        }\n",
    "        update_articles_t_v1_articals_groups_l1_many(updates: $updates) {\n",
    "            affected_rows\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_t_v1_articals_groups_l1']) == 0:\n",
    "            break\n",
    "        for response in response_data['data']['articles_t_v1_articals_groups_l1']:\n",
    "            llm_text = ''\n",
    "            articles_t_v1_articals_groups_l1_detail_insert_input=[]\n",
    "            articles_t_v1_articals_groups_l1_updates=[]\n",
    "            for article in response['articles_group']:\n",
    "                article_variables = {\n",
    "                \"article_id\": article\n",
    "                }\n",
    "                article_response_data = query_hasura_graphql(endpoint, admin_key, graphql_grquery_article, article_variables)\n",
    "                llm_text = llm_text + \"\\n\" +article_response_data['data']['articles_T_v1_rss1_articals'][0]['title'] + \"\\n\" + article_response_data['data']['articles_T_v1_rss1_articals'][0]['summary'] + \"\\n\" + article_response_data['data']['articles_T_v1_rss1_articals'][0]['T_v1_rss1_articles_detail']['summary']\n",
    "            texts = text_splitter.split_text(llm_text)\n",
    "            docs = [Document(page_content=t) for t in texts]\n",
    "            print(len(docs))\n",
    "            print(num_tokens_from_string(llm_text, model_name))\n",
    "            while True:\n",
    "                num_tokens = num_tokens_from_string(llm_text, model_name)\n",
    "                if num_tokens < gpt_35_turbo_max_tokens:\n",
    "                    break\n",
    "                else:\n",
    "                    docs = docs[:-1]\n",
    "            prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "            num_tokens = num_tokens_from_string(llm_text, model_name)\n",
    "            print(num_tokens)\n",
    "            if num_tokens < gpt_35_turbo_max_tokens:\n",
    "                chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=prompt, verbose=verbose)\n",
    "            else:\n",
    "                print(\"map reduce\")\n",
    "                chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt=prompt, combine_prompt=prompt, verbose=verbose)\n",
    "\n",
    "            start_time = monotonic()\n",
    "            summary = chain.run(docs)\n",
    "\n",
    "            #print(f\"Chain type: {chain.__class__.__name__}\")\n",
    "            #print(f\"Run time: {monotonic() - start_time}\")\n",
    "            #print(f\"Summary: {textwrap.fill(summary, width=100)}\")\n",
    "            articles_t_v1_articals_groups_l1_detail_insert_input.append({\n",
    "                        \"article_group_id\": response['id'],\n",
    "                        'summary': summary\n",
    "                        }\n",
    "                        )\n",
    "            articles_t_v1_articals_groups_l1_updates.append({\n",
    "                        \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                        \"_set\": {\"requires_update\": 1}\n",
    "                        })\n",
    "            mutation_variables = {\n",
    "            \"objects\": articles_t_v1_articals_groups_l1_detail_insert_input,\n",
    "            \"updates\": articles_t_v1_articals_groups_l1_updates,\n",
    "            }\n",
    "            out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4883\n"
     ]
    }
   ],
   "source": [
    "#update_articles()\n",
    "#update_article_details(0)\n",
    "#summerizer(0)\n",
    "#vectorize(0)\n",
    "#grouping(0)\n",
    "#grouping_l1(0)\n",
    "gen_article()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f2afa66fb14ff0a2a1af43aecb4fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab29866b98542ba9930f0f1ca4e726d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c6a9848e87422992bbe99e32a994f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af7dffd8fa946c6bbe8030d04da9d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")\n",
    "graphql_query = '''\n",
    "query MyQuery($limit: Int!, $offset: Int!) {\n",
    "        articles_t_v1_articals_groups_l1_detail(where: {title: {_is_null: true}}, limit: $limit, offset: $offset) {\n",
    "        article_group_id\n",
    "        summary\n",
    "    t_v1_articals_groups_l1 {\n",
    "      articles_group\n",
    "    }\n",
    "    }\n",
    "    }\n",
    "'''\n",
    "offset = 0\n",
    "graphql_query2 = '''\n",
    "query MyQuery($limit: Int!, $offset: Int!) {\n",
    "        articles_t_v1_articals_groups_l1_detail(where: {title: {_is_null: true}}, limit: $limit, offset: $offset) {\n",
    "        article_group_id\n",
    "        summary\n",
    "    }\n",
    "    }\n",
    "'''\n",
    "mutation_query= \"\"\"\n",
    "mutation MyMutation($updates: [articles_T_v1_rss1_articles_detail_updates!] = {where: {}}) {\n",
    "  update_articles_T_v1_rss1_articles_detail_many(updates: $updates) {\n",
    "    affected_rows\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "variables = {\n",
    "    \"limit\": 1,\n",
    "    \"offset\": offset\n",
    "    }\n",
    "\n",
    "response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "for response in response_data['data']['articles_t_v1_articals_groups_l1_detail']:\n",
    "    articles_T_v1_rss1_articles_detail_updates=[]\n",
    "    input_text = \"generate a intresting and viral news title for:  \" + \"\\n\" + response['summary']\n",
    "    max_length = 512\n",
    "    input_text = input_text[:max_length]\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, max_new_tokens=50)\n",
    "    generated_text = tokenizer.decode(outputs[0])\n",
    "    clean_text = re.sub('<.*?>', '', generated_text) # remove data between < and >\n",
    "    print(clean_text)\n",
    "    articles_T_v1_rss1_articles_detail_updates.append({\n",
    "        \"where\": {\"article_group_id\" : { \"_eq\": response['article_group_id'] }},\n",
    "        \"_set\": {\"title\": clean_text}\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ukraine and Russia Exchange Attacks in Crimea and Southern Ukraine\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")\n",
    "\n",
    "input_text = \"\"\"\n",
    "generate a viral news article for \n",
    "\n",
    "{summary}\n",
    "\"\"\"\n",
    "max_length = 512\n",
    "input_text = input_text[:max_length]\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(input_ids, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rss1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
