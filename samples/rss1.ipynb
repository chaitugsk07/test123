{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import feedparser\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "import pyperclip\n",
    "import time\n",
    "from transformers import pipeline\n",
    "import numpy\n",
    "import torch\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import json \n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "import tiktoken\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "from time import monotonic\n",
    "\n",
    "#git add . && git commit -m \"initial commit\" && git push origin main\n",
    "\n",
    "endpoint = \"https://enabling-elk-81.hasura.app/v1/graphql\"\n",
    "admin_key = \"bdAHRgu0lLGgF38TkQ0eL3ynNGLC23jxB4tnFzMiiSFh94YVMMHiIIouK4YfnEoB\"\n",
    "\n",
    "def query_hasura_graphql(endpoint, admin_key, query, variables):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'x-hasura-admin-secret': f'{admin_key}'\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'query': query,\n",
    "        'variables': variables\n",
    "    }\n",
    "    response = requests.post(endpoint, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def is_valid_timezone_format(published):\n",
    "    try:\n",
    "        # Attempt to parse the string\n",
    "        date_format = \"%a, %d %b %Y %H:%M:%S %z\"\n",
    "        date_object = datetime.strptime(published, date_format)\n",
    "        \n",
    "        hasura_timestamp = date_object.astimezone(timezone.utc).isoformat()\n",
    "        return True, hasura_timestamp\n",
    "    except ValueError:\n",
    "        # If parsing fails, the string is not in the correct format\n",
    "        return False, None\n",
    "\n",
    "def check_date_format(date_string):\n",
    "    try:\n",
    "        datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%S%z')\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "        \n",
    "def mutation_hasura_graphql(endpoint, admin_key, mutation_query, mutation_variables):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'x-hasura-admin-secret': f'{admin_key}'\n",
    "    }\n",
    "    response = requests.post(endpoint, json={'query': mutation_query, 'variables': mutation_variables}, headers=headers)\n",
    "    if response.ok:\n",
    "        data = response.json()\n",
    "        print(data)\n",
    "        return True, data\n",
    "    else:\n",
    "        print(f\"Mutation failed with status code {response.status_code}: {response.text}\")\n",
    "        return False, None\n",
    "\n",
    "def update_articles():\n",
    "    graphql_query = '''\n",
    "    query MyQuery($link_type: Int!) {\n",
    "        articles_t_v1_rss1_feed_links(where: {rss1_link_type: {_eq: $link_type}}) {\n",
    "            rss1_link\n",
    "            outlet\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    # Define the variables dictionary\n",
    "    variables = {\n",
    "        \"link_type\": 11\n",
    "    }\n",
    "    rss1_links_array = []\n",
    "    outlet = []\n",
    "    response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "    if response_data:\n",
    "        rss1_links_array = [item[\"rss1_link\"] for item in response_data[\"data\"][\"articles_t_v1_rss1_feed_links\"]]\n",
    "        outlet = [item[\"outlet\"] for item in response_data[\"data\"][\"articles_t_v1_rss1_feed_links\"]]\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_T_v1_rss1_articals_insert_input!] = {}) {\n",
    "        insert_articles_T_v1_rss1_articals(objects: $objects, on_conflict: {constraint: T_v1_rss1_articals_post_link_key}) {\n",
    "            affected_rows\n",
    "            returning {\n",
    "            id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0,len(rss1_links_array)-1):\n",
    "        NewsFeed = feedparser.parse(rss1_links_array[i])\n",
    "        print(\"############################################################\")\n",
    "        print(rss1_links_array[i])\n",
    "        articles = []\n",
    "        for entry in NewsFeed.entries:\n",
    "            # print(entry.link)\n",
    "            is_default_image = 0\n",
    "            title = entry.title\n",
    "            summary = ''\n",
    "            if 'summary' in entry:\n",
    "                summary_nofil = entry.summary\n",
    "                summary = re.sub('<[^<]+?>', '', summary_nofil)\n",
    "            image_url = \"\"\n",
    "            if 'media_content' in entry:\n",
    "                image_url = entry['media_content'][0]['url']\n",
    "                is_default_image = 1\n",
    "            if 'links' in entry:\n",
    "                for link in entry.links:\n",
    "                    if link.type == \"image/jpeg\":\n",
    "                        image_url= link.href\n",
    "                        is_default_image = 1\n",
    "                        break\n",
    "            post_link = entry.link\n",
    "            published = datetime.now(timezone.utc).isoformat()\n",
    "            if 'published' in entry:\n",
    "                published = entry.published\n",
    "            datevalidation = is_valid_timezone_format(published)\n",
    "            if datevalidation[0]:\n",
    "                hasura_timestamp = datevalidation[1]\n",
    "            if check_date_format(published):\n",
    "                hasura_timestamp = published\n",
    "            else:\n",
    "                hasura_timestamp = datetime.now().astimezone(timezone.utc).isoformat()\n",
    "            if \"author\" in entry:\n",
    "                author = entry.author\n",
    "            else:\n",
    "                author = \"na\"\n",
    "            if outlet[i] in post_link:\n",
    "                articles.append({\n",
    "                        \"rss1_link\": rss1_links_array[i],\n",
    "                        \"post_link\": post_link,\n",
    "                        \"title\": title,\n",
    "                        \"summary\": summary,\n",
    "                        \"author\": author,\n",
    "                        \"image_link\" : image_url,\n",
    "                        \"post_published\": hasura_timestamp,\n",
    "                        \"is_default_image\": is_default_image,\n",
    "                    }\n",
    "                )\n",
    "            #print(feed_link, post_link, title, summary, author, image_url, hasura_timestamp, is_default_image)\n",
    "        mutation_variables = {\n",
    "            \"objects\": articles\n",
    "        }\n",
    "        #print({'query': mutation_query, 'variables': mutation_variables})\n",
    "        out1 = mutation_hasura_graphql(endpoint = endpoint, admin_key = admin_key, mutation_query = mutation_query, mutation_variables = mutation_variables)\n",
    "\n",
    "def update_article_details(offset1):\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    articles_T_v1_rss1_articals(limit: $limit, offset: $offset, where: {is_in_detail: {_eq: 0}}, order_by: {post_published: desc}) {\n",
    "        post_link\n",
    "        is_default_image\n",
    "        image_link\n",
    "        id\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_T_v1_rss1_articles_detail_insert_input!] = {}, $updates: [articles_T_v1_rss1_articals_updates!] = {where: {}}) {\n",
    "        insert_articles_T_v1_rss1_articles_detail(objects: $objects, on_conflict: {constraint: T_v1_rss1_articles_detail_article_id_key}) {\n",
    "            affected_rows\n",
    "        }\n",
    "        update_articles_T_v1_rss1_articals_many(updates: $updates) {\n",
    "            affected_rows\n",
    "        }\n",
    "        }\n",
    "\n",
    "    \"\"\"    \n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.use_chromium = True\n",
    "    options.page_load_strategy = 'eager'\n",
    "    options.add_argument('--enable-immersive-reader')\n",
    "    driver = webdriver.Edge(options=options)\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 2,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        #print(variables, response_data)\n",
    "        #print(response_data)\n",
    "        post_links_array = []\n",
    "        ids=[]\n",
    "        if response_data:\n",
    "            post_links_array = [item[\"post_link\"] for item in response_data[\"data\"][\"articles_T_v1_rss1_articals\"]]\n",
    "            is_default_image_array = [item[\"is_default_image\"] for item in response_data[\"data\"][\"articles_T_v1_rss1_articals\"]]\n",
    "            image_link_array = [item[\"image_link\"] for item in response_data[\"data\"][\"articles_T_v1_rss1_articals\"]]\n",
    "            ids=[item[\"id\"] for item in response_data[\"data\"][\"articles_T_v1_rss1_articals\"]]\n",
    "        articles_detail = []\n",
    "        articles_update = []\n",
    "        if len(post_links_array) == 0:\n",
    "            break\n",
    "        try:\n",
    "            for a in range(len(post_links_array)):\n",
    "                main_link = post_links_array[a]\n",
    "                print(main_link)\n",
    "                driver.get(main_link)\n",
    "                get_url = driver.current_url\n",
    "                read_link= \"read://\"+get_url\n",
    "                driver.get(read_link)\n",
    "                time.sleep(5)\n",
    "                ActionChains(driver).key_down(Keys.CONTROL).send_keys('a').key_up(Keys.CONTROL).perform()\n",
    "                ActionChains(driver).key_down(Keys.CONTROL).send_keys('c').key_up(Keys.CONTROL).perform()\n",
    "                text = pyperclip.paste()\n",
    "                text2 = text\n",
    "                text3 = text2.split('\\n')\n",
    "                text3 = [s.replace('\\r', '') for s in text3]\n",
    "                special_chars = set(\"!@#$%^&*()_+[]{}|;:'\\\",<>?\")\n",
    "                text4 = [s for s in text3 if len(s) > 0 and (s[0] not in special_chars or s[-1] not in special_chars)]\n",
    "                my_list = text4\n",
    "                if my_list[0] == \"Hmmmâ€¦ can't reach this page\":\n",
    "                    offset = offset + 1\n",
    "                    break\n",
    "                my_set = set()\n",
    "                desription = []\n",
    "                for item in my_list:\n",
    "                    if item not in my_set:\n",
    "                        desription.append(item)\n",
    "                        my_set.add(item)\n",
    "                #print(desription)\n",
    "                images_final = []\n",
    "                articles_detail.append({\n",
    "                    \"article_id\": ids[a],\n",
    "                    \"title\": desription[0],\n",
    "                    \"description\": desription[1:],\n",
    "                    \"image_link\": images_final,\n",
    "                }\n",
    "                )\n",
    "                if (is_default_image_array[a] == 0 and len(images_final) > 0):\n",
    "                    articles_update.append({\n",
    "                        \"where\": {\"post_link\" : { \"_eq\": main_link }},\n",
    "                        \"_set\": {\"is_in_detail\": 1 , \"image_link\": images_final[0], \"is_default_image\": 1}\n",
    "                    })\n",
    "                else:\n",
    "                    articles_update.append({\n",
    "                        \"where\": {\"post_link\" : { \"_eq\": main_link }},\n",
    "                        \"_set\": {\"is_in_detail\": 1}\n",
    "                    })\n",
    "                \n",
    "                #print(main_link, desription[0], desription[1:], images_final)\n",
    "            #print(articles_update)\n",
    "            mutation_variables = {\n",
    "            \"objects\": articles_detail,\n",
    "            \"updates\": articles_update,\n",
    "            }\n",
    "            out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "        except:\n",
    "            offset = offset + 1\n",
    "            mutation_variables = {\n",
    "            \"objects\": articles_detail,\n",
    "            \"updates\": articles_update,\n",
    "            }\n",
    "            out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "        \n",
    "    driver.quit() \n",
    "\n",
    "def summerizer(offset1): \n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    articles_T_v1_rss1_articles_detail(limit: $limit, offset: $offset, where: {summary: {_is_null: true}}) {\n",
    "        title\n",
    "        description\n",
    "        article_id\n",
    "        T_v1_rss1_artical {\n",
    "        title\n",
    "        summary\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($updates: [articles_T_v1_rss1_articles_detail_updates!] = {where: {}}) {\n",
    "    update_articles_T_v1_rss1_articles_detail_many(updates: $updates) {\n",
    "        affected_rows\n",
    "        returning {\n",
    "        id\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 2,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        rss1_articles_detail_updates = []\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_T_v1_rss1_articles_detail']) == 0:\n",
    "            break\n",
    "        for response in response_data['data']['articles_T_v1_rss1_articles_detail']:\n",
    "            print(response['title'])\n",
    "            article=\"\"\n",
    "            article = article + response['title'] + \" \" +  response['T_v1_rss1_artical']['title'] + \" \" +  response['T_v1_rss1_artical']['summary'] + ', '.join(response['description'])\n",
    "            chunks=[]\n",
    "            max_length = 0\n",
    "            min_length = 0\n",
    "            if len(article) < 1000:\n",
    "                max_length = 150\n",
    "                min_length = 100\n",
    "                chunks.append(article)\n",
    "            elif len(article) < 3000:\n",
    "                max_length = 300\n",
    "                min_length = 200\n",
    "                chunks.append(article)\n",
    "            elif len(article) < 4000:\n",
    "                max_length = 400\n",
    "                min_length = 250\n",
    "                chunks.append(article)\n",
    "            elif len(article) < 8000:\n",
    "                max_length = 200\n",
    "                min_length = 150\n",
    "                midpoint = len(article) // 2\n",
    "                chunks.append(article[:midpoint])\n",
    "                chunks.append(article[midpoint:])\n",
    "            else:\n",
    "                article=article[:8000]\n",
    "                max_length = 200\n",
    "                min_length = 150\n",
    "                midpoint = len(article) // 2\n",
    "                chunks.append(article[:midpoint])\n",
    "                chunks.append(article[midpoint:])\n",
    "\n",
    "            summerize=\"\"\n",
    "            for chunk in chunks:\n",
    "                summerize=summerize + summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']+ \" \"\n",
    "            if len(summerize) > 0:\n",
    "                rss1_articles_detail_updates.append({\n",
    "                    \"where\": {\"article_id\" : { \"_eq\": response['article_id'] }},\n",
    "                    \"_set\": {\"summary\": summerize }\n",
    "                })\n",
    "        mutation_variables = {\n",
    "            \"updates\": rss1_articles_detail_updates,\n",
    "            }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def vectorize(offset1):\n",
    "    model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    articles_T_v1_rss1_articals(limit: $limit, offset: $offset, where: {is_vectorized: {_eq: 0}, is_in_detail: {_eq: 1}}) {\n",
    "        id\n",
    "        title\n",
    "        summary\n",
    "        T_v1_rss1_articles_detail {\n",
    "        summary\n",
    "        tags\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_t_v1_rss1_article_vectors_insert_input!] = {}, $updates: [articles_T_v1_rss1_articals_updates!] = {where: {}}) {\n",
    "    insert_articles_t_v1_rss1_article_vectors(objects: $objects, on_conflict: {constraint: t_v1_rss1_article_vectors_article_id_key}) {\n",
    "        affected_rows\n",
    "        returning {\n",
    "        article_id\n",
    "        }\n",
    "    }\n",
    "    update_articles_T_v1_rss1_articals_many(updates: $updates) {\n",
    "        affected_rows\n",
    "        returning {\n",
    "        id\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        articles_vector1_insert_input_loc=[]\n",
    "        rss1_articals_updates_loc=[]\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_T_v1_rss1_articals']) == 0:\n",
    "            break\n",
    "        #print(json.dumps(response_data, indent=4))\n",
    "        s1= []\n",
    "        ids=[]\n",
    "        for response in response_data['data']['articles_T_v1_rss1_articals']:\n",
    "            article=\"\"\n",
    "            tags=\"\"\n",
    "            if (response['T_v1_rss1_articles_detail']['tags']) is None:\n",
    "                tags = \" \"\n",
    "            else:\n",
    "                tags = \", \".join(response['T_v1_rss1_articles_detail']['tags'])\n",
    "            article = article + response['title'] + \" \" +  response['summary'] + \" \" +  response['T_v1_rss1_articles_detail']['summary'] + tags\n",
    "            s1.append([['Represent the news article for custering and retrieval:  ', article]])\n",
    "            ids.append(response['id'])\n",
    "        embeddings = []\n",
    "        for s in s1:\n",
    "            list_embeddings = numpy.ravel(model.encode(s)).tolist()\n",
    "            embeddings.append(list_embeddings)\n",
    "        for i in range(0,len(ids)):\n",
    "            articles_vector1_insert_input_loc.append({\n",
    "                \"article_id\": ids[i],\n",
    "                \"vector1\": str(embeddings[i]),\n",
    "                }\n",
    "                )\n",
    "            rss1_articals_updates_loc.append({\n",
    "                \"where\": {\"id\" : { \"_eq\": ids[i] }},\n",
    "                \"_set\": {\"is_vectorized\": 1}\n",
    "                })\n",
    "\n",
    "        mutation_variables = {\n",
    "        \"objects\": articles_vector1_insert_input_loc,\n",
    "        \"updates\": rss1_articals_updates_loc,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def grouping(offset1):\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "        articles_T_v1_rss1_articals(limit: $limit, offset: $offset, where: {is_vectorized: {_eq: 1}, is_in_detail: {_eq: 1}, is_grouped: {_eq: 0}}) {\n",
    "            id\n",
    "            t_v1_rss1_article_vector {\n",
    "            vector1\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_t_v1_articles_groups_insert_input!] = {}, $updates: [articles_T_v1_rss1_articals_updates!] = {where: {}}) {\n",
    "        insert_articles_t_v1_articles_groups(objects: $objects, on_conflict: {constraint: t_v1_articles_groups_article_id_key}) {\n",
    "            affected_rows\n",
    "            returning {\n",
    "            article_id\n",
    "            }\n",
    "        }\n",
    "        update_articles_T_v1_rss1_articals_many(updates: $updates) {\n",
    "            affected_rows\n",
    "            returning {\n",
    "            id\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    \"\"\"\n",
    "    func_query = '''\n",
    "    query MyQuery($p_article_id: bigint!) {\n",
    "        articles_get_similar_articles_group(args: {p_article_id: $p_article_id}) {\n",
    "            article_id\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 20,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        articles_groups_insert_input_loc=[]\n",
    "        rss1_articals_updates_loc=[]\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_T_v1_rss1_articals']) == 0:\n",
    "            break\n",
    "        #print(json.dumps(response_data, indent=4))\n",
    "        s1= []\n",
    "        ids=[]\n",
    "        for response in response_data['data']['articles_T_v1_rss1_articals']:\n",
    "            func_variables = {\n",
    "                \"p_article_id\": response['id']\n",
    "                }\n",
    "            func_response_data = query_hasura_graphql(endpoint, admin_key, func_query, func_variables)\n",
    "            article_group = []\n",
    "            #print(json.dumps(func_response_data, indent=4))\n",
    "            if len(func_response_data['data']['articles_get_similar_articles_group']) > 0:\n",
    "                for func_response in func_response_data['data']['articles_get_similar_articles_group']:\n",
    "                    article_group.append(func_response['article_id'])\n",
    "            \n",
    "            articles_groups_insert_input_loc.append({\n",
    "                \"article_id\": response['id'],\n",
    "                \"initial_group\": article_group,\n",
    "                }\n",
    "                )\n",
    "            rss1_articals_updates_loc.append({\n",
    "                \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                \"_set\": {\"is_grouped\": 1}\n",
    "                })\n",
    "\n",
    "        mutation_variables = {\n",
    "        \"objects\": articles_groups_insert_input_loc,\n",
    "        \"updates\": rss1_articals_updates_loc,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def grouping_l1(offset1):\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "          articles_T_v1_rss1_articals(where: {is_grouped: {_eq: 1}}, limit: $limit, offset: $offset) {\n",
    "            id\n",
    "            t_v1_articles_group {\n",
    "            initial_group\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "        mutation MyMutation($objects: [articles_t_v1_articals_groups_l1_insert_input!] = {}, $updates: [articles_T_v1_rss1_articals_updates!] = {where: {}}, $updates1: [articles_t_v1_articals_groups_l1_updates!] = {where: {}}) {\n",
    "        insert_articles_t_v1_articals_groups_l1(objects: $objects) {\n",
    "            affected_rows\n",
    "        }\n",
    "        update_articles_T_v1_rss1_articals_many(updates: $updates) {\n",
    "            affected_rows\n",
    "        }\n",
    "        update_articles_t_v1_articals_groups_l1_many(updates: $updates1) {\n",
    "            affected_rows\n",
    "        }\n",
    "        }\n",
    "    \"\"\"\n",
    "    query2 = '''\n",
    "    query MyQuery($articleid: [bigint!] = [20]) {\n",
    "        articles_t_v1_articles_groups(where: {initial_group: {_contains: $articleid}}) {\n",
    "            article_id\n",
    "            initial_group\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    query3 = '''\n",
    "    query MyQuery($articleid: [bigint!] = [20]) {\n",
    "        articles_t_v1_articals_groups_l1(where: {articles_group: {_contains: $articleid}}) {\n",
    "            articles_group\n",
    "            id\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        articles_grouped_l1_insert_input_loc=[]\n",
    "        rss1_articals_updates_loc=[]\n",
    "        articles_grouped_l1_updates=[]\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_T_v1_rss1_articals']) == 0:\n",
    "            break\n",
    "        #print(json.dumps(response_data, indent=4))\n",
    "        for response in response_data['data']['articles_T_v1_rss1_articals']:\n",
    "\n",
    "            variables2 = {\n",
    "                \"articleid\": [response['id']]\n",
    "                }\n",
    "            func_response_data = query_hasura_graphql(endpoint, admin_key, query2, variables2)\n",
    "            articles_ids = []\n",
    "            print(response['id'])\n",
    "            if len(func_response_data['data']['articles_t_v1_articles_groups']) > 0:\n",
    "                for func_response in func_response_data['data']['articles_t_v1_articles_groups']:\n",
    "                    articles_ids.append(func_response['initial_group'])\n",
    "            \n",
    "            func_response_data1 = query_hasura_graphql(endpoint, admin_key, query3, variables2)\n",
    "            \n",
    "            \n",
    "            if (len(func_response_data1['data']['articles_t_v1_articals_groups_l1']) == 0):\n",
    "                new_lst = []\n",
    "                for sublist in articles_ids:\n",
    "                    for element in sublist:\n",
    "                        new_lst.append(element)\n",
    "                my_list = list(set(new_lst))\n",
    "                print(my_list)\n",
    "                articles_grouped_l1_insert_input_loc.append({\n",
    "                    \"articles_group\": my_list,\n",
    "                    'articles_in_group': len(my_list)\n",
    "                    }\n",
    "                    )\n",
    "                rss1_articals_updates_loc.append({\n",
    "                    \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                    \"_set\": {\"is_grouped\": 2}\n",
    "                    })\n",
    "            else:\n",
    "                articles_ids.append(func_response_data1['data']['articles_t_v1_articals_groups_l1'][0]['articles_group'])\n",
    "                new_lst = []\n",
    "                for sublist in articles_ids:\n",
    "                    for element in sublist:\n",
    "                        new_lst.append(element)\n",
    "                my_list = list(set(new_lst))\n",
    "                articles_grouped_l1_updates.append({\n",
    "                    \"where\": {\"id\" : { \"_eq\": func_response_data1['data']['articles_t_v1_articals_groups_l1'][0]['id'] }},\n",
    "                    \"_set\": {\"articles_group\": my_list, 'articles_in_group': len(my_list)}\n",
    "                    })\n",
    "                rss1_articals_updates_loc.append({\n",
    "                    \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                    \"_set\": {\"is_grouped\": 2}\n",
    "                    })\n",
    "                print(my_list)     \n",
    "        \n",
    "        mutation_variables = {\n",
    "        \"objects\": articles_grouped_l1_insert_input_loc,\n",
    "        \"updates\": rss1_articals_updates_loc,\n",
    "        \"updates1\": articles_grouped_l1_updates,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:    \n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def gen_article():\n",
    "    gpt_35_turbo_max_tokens = 4097\n",
    "    verbose = True\n",
    "    prompt_template = \"\"\"Write a unbiased professional news article for:\n",
    "\n",
    "\n",
    "        {text}\n",
    "\n",
    "\n",
    "        CONSCISE UNBIASED detailed news article with at least 500 words:\"\"\"\n",
    "    OPENAI_API_KEY= ''\n",
    "    model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model_name=model_name)\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "        articles_t_v1_articals_groups_l1(where: {requires_update: {_eq: 0}, articles_in_group: {_gt: 1}}, limit: $limit, offset: $offset) {\n",
    "            articles_group\n",
    "            id\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    graphql_grquery_article = '''\n",
    "    query MyQuery($article_id: bigint!) {\n",
    "    articles_T_v1_rss1_articals(where: {id: {_eq: $article_id}}) {\n",
    "        title\n",
    "        summary\n",
    "        T_v1_rss1_articles_detail {\n",
    "        summary\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    offset = 0\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_t_v1_articals_groups_l1_detail_insert_input!] = {}, $updates: [articles_t_v1_articals_groups_l1_updates!] = {where: {}}) {\n",
    "        insert_articles_t_v1_articals_groups_l1_detail(objects: $objects, on_conflict: {constraint: t_v1_articals_groups_l1_detail_article_group_id_key}) {\n",
    "            affected_rows\n",
    "        }\n",
    "        update_articles_t_v1_articals_groups_l1_many(updates: $updates) {\n",
    "            affected_rows\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_t_v1_articals_groups_l1']) == 0:\n",
    "            break\n",
    "        for response in response_data['data']['articles_t_v1_articals_groups_l1']:\n",
    "            llm_text = ''\n",
    "            articles_t_v1_articals_groups_l1_detail_insert_input=[]\n",
    "            articles_t_v1_articals_groups_l1_updates=[]\n",
    "            for article in response['articles_group']:\n",
    "                article_variables = {\n",
    "                \"article_id\": article\n",
    "                }\n",
    "                article_response_data = query_hasura_graphql(endpoint, admin_key, graphql_grquery_article, article_variables)\n",
    "                llm_text = llm_text + \"\\n\" +article_response_data['data']['articles_T_v1_rss1_articals'][0]['title'] + \"\\n\" + article_response_data['data']['articles_T_v1_rss1_articals'][0]['summary'] + \"\\n\" + article_response_data['data']['articles_T_v1_rss1_articals'][0]['T_v1_rss1_articles_detail']['summary']\n",
    "            max_tokens = 3000\n",
    "            if len(llm_text.split()) > max_tokens:\n",
    "                llm_text = ' '.join(llm_text.split()[:max_tokens])\n",
    "            print(num_tokens_from_string(llm_text, model_name))\n",
    "            \n",
    "            texts = text_splitter.split_text(llm_text)\n",
    "            docs = [Document(page_content=t) for t in texts]\n",
    "            max_tokens = 4000\n",
    "            if len(llm_text.split()) > max_tokens:\n",
    "                llm_text = ' '.join(llm_text.split()[:max_tokens])\n",
    "            prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "            num_tokens = num_tokens_from_string(llm_text, model_name)\n",
    "            print(num_tokens)\n",
    "            if num_tokens < gpt_35_turbo_max_tokens:\n",
    "                chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=prompt)\n",
    "            else:\n",
    "                print(\"map reduce\")\n",
    "                chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt=prompt, combine_prompt=prompt, verbose=verbose)\n",
    "\n",
    "            summary = chain.run(docs)\n",
    "\n",
    "            #print(f\"Chain type: {chain.__class__.__name__}\")\n",
    "            #print(f\"Run time: {monotonic() - start_time}\")\n",
    "            #print(f\"Summary: {textwrap.fill(summary, width=100)}\")\n",
    "            articles_t_v1_articals_groups_l1_detail_insert_input.append({\n",
    "                        \"article_group_id\": response['id'],\n",
    "                        'summary': summary\n",
    "                        }\n",
    "                        )\n",
    "            articles_t_v1_articals_groups_l1_updates.append({\n",
    "                        \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                        \"_set\": {\"requires_update\": 1}\n",
    "                        })\n",
    "            mutation_variables = {\n",
    "            \"objects\": articles_t_v1_articals_groups_l1_detail_insert_input,\n",
    "            \"updates\": articles_t_v1_articals_groups_l1_updates,\n",
    "            }\n",
    "            out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def gen_title():\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "            articles_t_v1_articals_groups_l1_detail(where: {title: {_is_null: true}}, limit: $limit, offset: $offset) {\n",
    "            article_group_id\n",
    "            summary\n",
    "        t_v1_articals_groups_l1 {\n",
    "        articles_group\n",
    "        }\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    offset = 0\n",
    "    mutation_query= \"\"\"\n",
    "    mutation MyMutation($updates: [articles_t_v1_articals_groups_l1_detail_updates!] = {where: {}}) {\n",
    "  update_articles_t_v1_articals_groups_l1_detail_many(updates: $updates) {\n",
    "    affected_rows\n",
    "  }\n",
    "}\n",
    "\n",
    "    \"\"\"\n",
    "    variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "    while True:\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_t_v1_articals_groups_l1_detail']) == 0:\n",
    "            break\n",
    "        update_articles_t_v1_articals_groups_l1_detail_many_loc=[]\n",
    "        for response in response_data['data']['articles_t_v1_articals_groups_l1_detail']:\n",
    "            input_text = \"generate a intresting and viral news title for:  \" + \"\\n\" + response['summary']\n",
    "            max_length = 512\n",
    "            input_text = input_text[:max_length]\n",
    "            input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "            outputs = model.generate(input_ids, max_new_tokens=50)\n",
    "            generated_text = tokenizer.decode(outputs[0])\n",
    "            clean_text = re.sub('<.*?>', '', generated_text) # remove data between < and >\n",
    "            print(clean_text)\n",
    "            update_articles_t_v1_articals_groups_l1_detail_many_loc.append({\n",
    "                \"where\": {\"article_group_id\" : { \"_eq\": response['article_group_id'] }},\n",
    "                \"_set\": {\"title\": clean_text}\n",
    "                })\n",
    "        mutation_variables = {\n",
    "            \"updates\": update_articles_t_v1_articals_groups_l1_detail_many_loc,\n",
    "            }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def gen_images():\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "        articles_t_v1_articals_groups_l1_detail(where: {image_urls: {_is_null: true}, _and: {logo_urls: {_is_null: true}}}, limit: $limit, offset: $offset) {\n",
    "            article_group_id\n",
    "            id\n",
    "            t_v1_articals_groups_l1 {\n",
    "            articles_group\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    offset = 0\n",
    "    graphql_query2 = '''\n",
    "    query MyQuery($article_id: bigint! ) {\n",
    "    articles_T_v1_rss1_articals(where: {id: {_eq: $article_id}}) {\n",
    "        image_link\n",
    "        t_v1_rss1_feed_link {\n",
    "        t_v1_outlet {\n",
    "            logo_url\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    mutation_query= \"\"\"\n",
    "    mutation MyMutation($updates: [articles_t_v1_articals_groups_l1_detail_updates!] = {where: {}}) {\n",
    "    update_articles_t_v1_articals_groups_l1_detail_many(updates: $updates) {\n",
    "    affected_rows\n",
    "    }\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "    while True:\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        #print(json.dumps(response_data, indent=4))\n",
    "        if len(response_data['data']['articles_t_v1_articals_groups_l1_detail']) == 0:\n",
    "            break\n",
    "        articles_t_v1_articals_groups_l1_detail_updates_loc=[]\n",
    "        for response in response_data['data']['articles_t_v1_articals_groups_l1_detail']:\n",
    "            #print(json.dumps(response, indent=4))\n",
    "            image_links=[]\n",
    "            logo_links=[]\n",
    "            for article_id in response['t_v1_articals_groups_l1']['articles_group']:\n",
    "                variables2 = {\n",
    "                    \"article_id\": article_id\n",
    "                    }\n",
    "                response_data_article = query_hasura_graphql(endpoint, admin_key, graphql_query2, variables2)\n",
    "                if response_data_article['data']['articles_T_v1_rss1_articals'][0]['image_link'] != '':\n",
    "                    image_links.append(response_data_article['data']['articles_T_v1_rss1_articals'][0]['image_link'])\n",
    "                if response_data_article['data']['articles_T_v1_rss1_articals'][0]['t_v1_rss1_feed_link']['t_v1_outlet']['logo_url'] != '':\n",
    "                    logo_links.append(response_data_article['data']['articles_T_v1_rss1_articals'][0]['t_v1_rss1_feed_link']['t_v1_outlet']['logo_url'])\n",
    "            #print(image_links)\n",
    "            #print(logo_links)\n",
    "            image_links = list(set(image_links))\n",
    "            logo_links = list(set(logo_links))\n",
    "            articles_t_v1_articals_groups_l1_detail_updates_loc.append({\n",
    "                \"where\": {\"article_group_id\" : { \"_eq\": response['article_group_id'] }},\n",
    "                \"_set\": {\"image_urls\": image_links, \"logo_urls\": logo_links}\n",
    "                })  \n",
    "        mutation_variables = {\n",
    "            \"updates\": articles_t_v1_articals_groups_l1_detail_updates_loc,\n",
    "            }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def summerizer_60_words(offset1): \n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    articles_t_v1_articals_groups_l1_detail(limit: $limit, offset: $offset, where: {summary_60_words: {_is_null: true}}) {\n",
    "        article_group_id\n",
    "        summary\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($updates: [articles_t_v1_articals_groups_l1_detail_updates!] = {where: {}}) {\n",
    "    update_articles_t_v1_articals_groups_l1_detail_many (updates: $updates) {\n",
    "        affected_rows\n",
    "        returning {\n",
    "        id\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 2,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        articles_t_v1_articals_groups_l1_detail_updates_loc = []\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_t_v1_articals_groups_l1_detail']) == 0:\n",
    "            break\n",
    "        for response in response_data['data']['articles_t_v1_articals_groups_l1_detail']:\n",
    "            print(response['article_group_id'])\n",
    "            article = response['summary'] \n",
    "            max_length = 65\n",
    "            min_length = 45\n",
    "            \n",
    "            summerize= summarizer(article, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']+ \" \"\n",
    "            if len(summerize) > 0:\n",
    "                articles_t_v1_articals_groups_l1_detail_updates_loc.append({\n",
    "                    \"where\": {\"article_group_id\" : { \"_eq\": response['article_group_id'] }},\n",
    "                    \"_set\": {\"summary_60_words\": summerize }\n",
    "                })\n",
    "        mutation_variables = {\n",
    "            \"updates\": articles_t_v1_articals_groups_l1_detail_updates_loc,\n",
    "            }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#update_articles()\n",
    "#update_article_details(0)\n",
    "#summerizer(0)\n",
    "#vectorize(0)\n",
    "#grouping(0)\n",
    "#grouping_l1(0)\n",
    "#gen_article()\n",
    "#gen_title()\n",
    "gen_images()\n",
    "summerizer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gskch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "endpoint = \"https://enabling-elk-81.hasura.app/v1/graphql\"\n",
    "admin_key = \"bdAHRgu0lLGgF38TkQ0eL3ynNGLC23jxB4tnFzMiiSFh94YVMMHiIIouK4YfnEoB\"\n",
    "\n",
    "\n",
    "def query_hasura_graphql(endpoint, admin_key, query, variables):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'x-hasura-admin-secret': f'{admin_key}'\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'query': query,\n",
    "        'variables': variables\n",
    "    }\n",
    "    response = requests.post(endpoint, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def mutation_hasura_graphql(endpoint, admin_key, mutation_query, mutation_variables):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'x-hasura-admin-secret': f'{admin_key}'\n",
    "    }\n",
    "    response = requests.post(endpoint, json={'query': mutation_query, 'variables': mutation_variables}, headers=headers)\n",
    "    if response.ok:\n",
    "        data = response.json()\n",
    "        print(data)\n",
    "        return True, data\n",
    "    else:\n",
    "        print(f\"Mutation failed with status code {response.status_code}: {response.text}\")\n",
    "        return False, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "graphql_query = '''\n",
    "query MyQuery($limit: Int!, $offset: Int!) {\n",
    "articles_t_v1_articals_groups_l1_detail(where: {is_short_created_valid: {_eq: 0}}, limit: $limit, offset: $offset) {\n",
    "    summary_60_words\n",
    "    article_group_id\n",
    "    summary\n",
    "    title\n",
    "  }\n",
    "}\n",
    "'''\n",
    "offset = 0\n",
    "variables = {\n",
    "\"limit\": 1,\n",
    "\"offset\": offset\n",
    "}\n",
    "response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "video_text=[]\n",
    "for response in response_data['data']['articles_t_v1_articals_groups_l1_detail']:\n",
    "  summary = (response['summary'])\n",
    "  sentences = nltk.sent_tokenize(summary)\n",
    "  next_int = math.ceil(len(sentences)/5)\n",
    "  for i in range (0,5):\n",
    "    sliced_sentences = sentences[next_int*i:next_int*(i+1)]\n",
    "    sum_sent = \" \".join(sliced_sentences)\n",
    "    input_text = \"generate a short intresting and viral news title for:  \" + \"\\n\" + sum_sent\n",
    "    max_length = 512\n",
    "    input_text = input_text[:max_length]\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids, max_new_tokens=30)\n",
    "    generated_text = tokenizer.decode(outputs[0])\n",
    "    clean_text = re.sub('<.*?>', '', generated_text)\n",
    "    video_text.append(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video generated/output.mp4.\n",
      "Moviepy - Writing video generated/output.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready generated/output.mp4\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "import moviepy.video.fx.crop as crop_vid\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from moviepy.video.VideoClip import TextClip, ImageClip\n",
    "from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip\n",
    "from moviepy.video.fx import all as vfx\n",
    "\n",
    "\n",
    "def download_image(url):\n",
    "    if not os.path.exists('temp'):\n",
    "        os.makedirs('temp')\n",
    "    path_components = os.path.splitext(url)\n",
    "    filename = os.path.basename(path_components[0])\n",
    "    filename = filename + url[-4:]\n",
    "    response = requests.get(url)\n",
    "    image_data = response.content\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "    file_path = os.path.join('temp', filename)\n",
    "    image.save(file_path)\n",
    "    return file_path\n",
    "\n",
    "if os.path.exists('generated') == False:\n",
    "    os.mkdir('generated')\n",
    "\n",
    "bgimage_path = \"https://i.pinimg.com/564x/ad/c6/85/adc685fee66ede448182584111b70292.jpg\"\n",
    "background = ImageClip(bgimage_path)\n",
    "logo_path = \"https://i.postimg.cc/x1fnjWpF/logo.png\"\n",
    "aspect_ratio = (9, 16)\n",
    "resolution = (1920, 1080)\n",
    "background = background.resize(height=resolution[1], width=int(resolution[1] * aspect_ratio[0] / aspect_ratio[1]))\n",
    "\n",
    "logo = ImageClip(download_image(logo_path))\n",
    "logo = logo.resize(height=100, width=100).set_position((0.2,0.007), relative=True)\n",
    "\n",
    "\n",
    "top_text = TextClip(\"synopse\", fontsize=50, color='white', font='Arial', align='center').set_position(('center', 0.02), relative=True)\n",
    "bottom_text = TextClip(\"synopse 1\", fontsize=25, color='white', font='Arial', align='center').set_position(('center', 0.9), relative=True)\n",
    "composite_clip = CompositeVideoClip([background, logo, top_text, bottom_text])\n",
    "composite_clip = composite_clip.set_duration(10).set_fps(30)\n",
    "\n",
    "top_text = top_text.set_start(3).set_end(8)\n",
    "\n",
    "\n",
    "# Create the composite video clip\n",
    "composite_clip = CompositeVideoClip([background, logo, top_text, bottom_text])\n",
    "\n",
    "# Set the composite clip's duration and frame rate\n",
    "composite_clip = composite_clip.set_duration(10).set_fps(30)\n",
    "\n",
    "composite_clip.write_videofile(\"generated/output.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 235/486 [00:21<00:20, 11.99it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video generated/output.mp4.\n",
      "MoviePy - Writing audio in outputTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 235/486 [00:22<00:20, 11.99it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video generated/output.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 235/486 [00:30<00:20, 11.99it/s, now=None]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\D_drive\\11_synopse\\30_git\\test123\\samples\\rss1.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/D_drive/11_synopse/30_git/test123/samples/rss1.ipynb#W5sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m composite_clip \u001b[39m=\u001b[39m CompositeVideoClip(video)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/D_drive/11_synopse/30_git/test123/samples/rss1.ipynb#W5sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m composite_clip \u001b[39m=\u001b[39m composite_clip\u001b[39m.\u001b[39mset_duration(audio_duration)\u001b[39m.\u001b[39mset_fps(\u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mset_audio(audioclip)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/D_drive/11_synopse/30_git/test123/samples/rss1.ipynb#W5sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m composite_clip\u001b[39m.\u001b[39;49mwrite_videofile(\u001b[39m\"\u001b[39;49m\u001b[39mgenerated/output.mp4\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32m<decorator-gen-73>:2\u001b[0m, in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\gskch\\anaconda3\\envs\\rss1\\Lib\\site-packages\\moviepy\\decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAttribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m'\u001b[39m\u001b[39m not set\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m f(clip, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n",
      "File \u001b[1;32m<decorator-gen-72>:2\u001b[0m, in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\gskch\\anaconda3\\envs\\rss1\\Lib\\site-packages\\moviepy\\decorators.py:135\u001b[0m, in \u001b[0;36muse_clip_fps_by_default\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m    130\u001b[0m new_a \u001b[39m=\u001b[39m [fun(arg) \u001b[39mif\u001b[39;00m (name\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfps\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m arg\n\u001b[0;32m    131\u001b[0m          \u001b[39mfor\u001b[39;00m (arg, name) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(a, names)]\n\u001b[0;32m    132\u001b[0m new_kw \u001b[39m=\u001b[39m {k: fun(v) \u001b[39mif\u001b[39;00m k\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfps\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m v\n\u001b[0;32m    133\u001b[0m          \u001b[39mfor\u001b[39;00m (k,v) \u001b[39min\u001b[39;00m k\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m--> 135\u001b[0m \u001b[39mreturn\u001b[39;00m f(clip, \u001b[39m*\u001b[39;49mnew_a, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kw)\n",
      "File \u001b[1;32m<decorator-gen-71>:2\u001b[0m, in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\gskch\\anaconda3\\envs\\rss1\\Lib\\site-packages\\moviepy\\decorators.py:22\u001b[0m, in \u001b[0;36mconvert_masks_to_RGB\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m clip\u001b[39m.\u001b[39mismask:\n\u001b[0;32m     21\u001b[0m     clip \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mto_RGB()\n\u001b[1;32m---> 22\u001b[0m \u001b[39mreturn\u001b[39;00m f(clip, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n",
      "File \u001b[1;32mc:\\Users\\gskch\\anaconda3\\envs\\rss1\\Lib\\site-packages\\moviepy\\video\\VideoClip.py:300\u001b[0m, in \u001b[0;36mVideoClip.write_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39mif\u001b[39;00m make_audio:\n\u001b[0;32m    293\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio\u001b[39m.\u001b[39mwrite_audiofile(audiofile, audio_fps,\n\u001b[0;32m    294\u001b[0m                                audio_nbytes, audio_bufsize,\n\u001b[0;32m    295\u001b[0m                                audio_codec, bitrate\u001b[39m=\u001b[39maudio_bitrate,\n\u001b[0;32m    296\u001b[0m                                write_logfile\u001b[39m=\u001b[39mwrite_logfile,\n\u001b[0;32m    297\u001b[0m                                verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    298\u001b[0m                                logger\u001b[39m=\u001b[39mlogger)\n\u001b[1;32m--> 300\u001b[0m ffmpeg_write_video(\u001b[39mself\u001b[39;49m, filename, fps, codec,\n\u001b[0;32m    301\u001b[0m                    bitrate\u001b[39m=\u001b[39;49mbitrate,\n\u001b[0;32m    302\u001b[0m                    preset\u001b[39m=\u001b[39;49mpreset,\n\u001b[0;32m    303\u001b[0m                    write_logfile\u001b[39m=\u001b[39;49mwrite_logfile,\n\u001b[0;32m    304\u001b[0m                    audiofile\u001b[39m=\u001b[39;49maudiofile,\n\u001b[0;32m    305\u001b[0m                    verbose\u001b[39m=\u001b[39;49mverbose, threads\u001b[39m=\u001b[39;49mthreads,\n\u001b[0;32m    306\u001b[0m                    ffmpeg_params\u001b[39m=\u001b[39;49mffmpeg_params,\n\u001b[0;32m    307\u001b[0m                    logger\u001b[39m=\u001b[39;49mlogger)\n\u001b[0;32m    309\u001b[0m \u001b[39mif\u001b[39;00m remove_temp \u001b[39mand\u001b[39;00m make_audio:\n\u001b[0;32m    310\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(audiofile):\n",
      "File \u001b[1;32mc:\\Users\\gskch\\anaconda3\\envs\\rss1\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_writer.py:228\u001b[0m, in \u001b[0;36mffmpeg_write_video\u001b[1;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[0;32m    225\u001b[0m                 mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    226\u001b[0m             frame \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdstack([frame,mask])\n\u001b[1;32m--> 228\u001b[0m         writer\u001b[39m.\u001b[39;49mwrite_frame(frame)\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m write_logfile:\n\u001b[0;32m    231\u001b[0m     logfile\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\gskch\\anaconda3\\envs\\rss1\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_writer.py:136\u001b[0m, in \u001b[0;36mFFMPEG_VideoWriter.write_frame\u001b[1;34m(self, img_array)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m PY3:\n\u001b[1;32m--> 136\u001b[0m        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproc\u001b[39m.\u001b[39mstdin\u001b[39m.\u001b[39mwrite(img_array\u001b[39m.\u001b[39mtobytes())\n\u001b[0;32m    137\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproc\u001b[39m.\u001b[39mstdin\u001b[39m.\u001b[39mwrite(img_array\u001b[39m.\u001b[39mtostring())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "text1 = ['Gets a numpy array representing the RGB picture of the clip at time t or (mono or stereo) value for a sound clip', \"Iterates over all the frames of the clip.\", \"The fps (frames per second) parameter is optional if the clip already has a fps attribute.\"]\n",
    "title_text = \"thi. we are still here s is movie Py is is the conclusion\"\n",
    "audio_path = \"temp/mine1.mp3\"\n",
    "conclusion_text = \"this is the conclusion of the video\"\n",
    "\n",
    "\n",
    "audioclip = AudioFileClip(audio_path)\n",
    "audio_duration = audioclip.duration\n",
    "title_duration = 5\n",
    "conclusion_duration = 5\n",
    "generate_duration = audio_duration - title_duration - conclusion_duration\n",
    "each_text_dur = generate_duration/len(text1)\n",
    "height = 0.6 / len(text1)\n",
    "video = []\n",
    "bgimage_path = \"https://i.pinimg.com/564x/ad/c6/85/adc685fee66ede448182584111b70292.jpg\"\n",
    "background = ImageClip(bgimage_path)\n",
    "logo_path = \"https://i.postimg.cc/x1fnjWpF/logo.png\"\n",
    "aspect_ratio = (9, 16)\n",
    "resolution = (1920, 1080)\n",
    "background = background.resize(height=resolution[1], width=int(resolution[1] * aspect_ratio[0] / aspect_ratio[1]))\n",
    "video.append(background)\n",
    "\n",
    "top_text = TextClip(\"synopse\", fontsize=50, color='white', font='Arial', align='center').set_position(('center', 0.02), relative=True)\n",
    "top_logo = ImageClip(download_image(logo_path)).resize(height=100, width=100).set_position((0.2,0.007), relative=True)\n",
    "video.append(top_text)\n",
    "video.append(top_logo)\n",
    "\n",
    "bottom_text = TextClip(\"synopse\", fontsize=25, color='white', font='Arial', align='center').set_position(('center', 0.95), relative=True)\n",
    "bottom_logo = ImageClip(download_image(logo_path)).resize(height=50, width=50).set_position((0.35,0.945), relative=True)\n",
    "video.append(bottom_text)\n",
    "video.append(bottom_logo)\n",
    "screensize = (500,100)\n",
    "screensize1 = (500,200)\n",
    "\n",
    "title_middle = TextClip(title_text, fontsize=35, color='white', font='Arial', align='center', method = 'caption', kerning=-2, interline=-1, size = screensize,).set_position(('center'), relative=True)\n",
    "title_middle = title_middle.set_start(1).set_end(5)\n",
    "\n",
    "video.append(title_middle)\n",
    "\n",
    "title_top= TextClip(title_text, fontsize=35, color='white', font='Arial', align='center', method = 'caption', kerning=-2, interline=-1, size = screensize,).set_position(('center', 0.085), relative=True)\n",
    "title_top = title_top.set_start(5).set_end(audio_duration - conclusion_duration)\n",
    "video.append(title_top)\n",
    "\n",
    "for i in range(0, len(text1)):\n",
    "    video.append(TextClip(text1[i], fontsize=30, color='white', font='Arial', align='center', method = 'caption', kerning=-2, interline=-1, size = screensize1,).set_position(('center', 0.2 + height * i ), relative=True).set_start(6+each_text_dur*i).set_end(audio_duration - conclusion_duration))\n",
    "    \n",
    "composite_clip = CompositeVideoClip(video)\n",
    "composite_clip = composite_clip.set_duration(audio_duration).set_fps(10).set_audio(audioclip)\n",
    "composite_clip.write_videofile(\"generated/output.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import random\n",
    "\n",
    "text1 = ['Gets a numpy array representing the RGB picture of the clip at time t or (mono or stereo) value for a sound clip', \"Iterates over all the frames of the clip.\", \"The fps (frames per second) parameter is optional if the clip already has a fps attribute.\"]\n",
    "title_text = \"thi. we are still here s is movie Py is is the conclusion\"\n",
    "audio_path = \"temp/mine1.mp3\"\n",
    "conclusion_text = \"this is the conclusion of the video\"\n",
    "\n",
    "audio_duration = 10\n",
    "title_duration = 5\n",
    "conclusion_duration = 5\n",
    "generate_duration = audio_duration - title_duration - conclusion_duration\n",
    "each_text_dur = generate_duration/len(text1)\n",
    "video = []\n",
    "image_files = [f for f in os.listdir(\"background\") if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "image_file = random.choice(image_files)\n",
    "bg_image = \"background/\" + image_file\n",
    "output_video = \"generated/output.mp4\"\n",
    "\n",
    "new_height = 1080\n",
    "new_width = new_height * 9 / 16\n",
    "\n",
    "\n",
    "# Run the ffmpeg command to convert the image to a 10 second video\n",
    "subprocess.run(['ffmpeg', '-loop', '1', '-i', bg_image, '-c:v', 'libx264', '-t', '10', '-pix_fmt', 'yuv420p', output_video])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import feedparser\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "import pyperclip\n",
    "import time\n",
    "from transformers import pipeline\n",
    "import numpy\n",
    "import torch\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import json \n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "import tiktoken\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "from time import monotonic\n",
    "\n",
    "#git add . && git commit -m \"initial commit\" && git push origin main\n",
    "\n",
    "endpoint = \"https://enabling-elk-81.hasura.app/v1/graphql\"\n",
    "admin_key = \"bdAHRgu0lLGgF38TkQ0eL3ynNGLC23jxB4tnFzMiiSFh94YVMMHiIIouK4YfnEoB\"\n",
    "\n",
    "def query_hasura_graphql(endpoint, admin_key, query, variables):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'x-hasura-admin-secret': f'{admin_key}'\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'query': query,\n",
    "        'variables': variables\n",
    "    }\n",
    "    response = requests.post(endpoint, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def is_valid_timezone_format(published):\n",
    "    try:\n",
    "        # Attempt to parse the string\n",
    "        date_format = \"%a, %d %b %Y %H:%M:%S %z\"\n",
    "        date_object = datetime.strptime(published, date_format)\n",
    "        \n",
    "        hasura_timestamp = date_object.astimezone(timezone.utc).isoformat()\n",
    "        return True, hasura_timestamp\n",
    "    except ValueError:\n",
    "        # If parsing fails, the string is not in the correct format\n",
    "        return False, None\n",
    "\n",
    "def check_date_format(date_string):\n",
    "    try:\n",
    "        datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%S%z')\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "        \n",
    "def mutation_hasura_graphql(endpoint, admin_key, mutation_query, mutation_variables):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'x-hasura-admin-secret': f'{admin_key}'\n",
    "    }\n",
    "    response = requests.post(endpoint, json={'query': mutation_query, 'variables': mutation_variables}, headers=headers)\n",
    "    if response.ok:\n",
    "        data = response.json()\n",
    "        print(data)\n",
    "        return True, data\n",
    "    else:\n",
    "        print(f\"Mutation failed with status code {response.status_code}: {response.text}\")\n",
    "        return False, None\n",
    "\n",
    "def update_articles():\n",
    "    graphql_query = '''\n",
    "    query MyQuery($link_type: Int!) {\n",
    "        articles_t_v1_rss1_feed_links(where: {rss1_link_type: {_eq: $link_type}}) {\n",
    "            rss1_link\n",
    "            outlet\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    # Define the variables dictionary\n",
    "    variables = {\n",
    "        \"link_type\": 11\n",
    "    }\n",
    "    rss1_links_array = []\n",
    "    outlet = []\n",
    "    response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "    if response_data:\n",
    "        rss1_links_array = [item[\"rss1_link\"] for item in response_data[\"data\"][\"articles_t_v1_rss1_feed_links\"]]\n",
    "        outlet = [item[\"outlet\"] for item in response_data[\"data\"][\"articles_t_v1_rss1_feed_links\"]]\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_T_v1_rss1_articals_insert_input!] = {}) {\n",
    "        insert_articles_T_v1_rss1_articals(objects: $objects, on_conflict: {constraint: T_v1_rss1_articals_post_link_key}) {\n",
    "            affected_rows\n",
    "            returning {\n",
    "            id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0,len(rss1_links_array)-1):\n",
    "        NewsFeed = feedparser.parse(rss1_links_array[i])\n",
    "        print(\"############################################################\")\n",
    "        print(rss1_links_array[i])\n",
    "        articles = []\n",
    "        for entry in NewsFeed.entries:\n",
    "            # print(entry.link)\n",
    "            is_default_image = 0\n",
    "            title = entry.title\n",
    "            summary = ''\n",
    "            if 'summary' in entry:\n",
    "                summary_nofil = entry.summary\n",
    "                summary = re.sub('<[^<]+?>', '', summary_nofil)\n",
    "            image_url = \"\"\n",
    "            if 'media_content' in entry:\n",
    "                image_url = entry['media_content'][0]['url']\n",
    "                is_default_image = 1\n",
    "            if 'links' in entry:\n",
    "                for link in entry.links:\n",
    "                    if link.type == \"image/jpeg\":\n",
    "                        image_url= link.href\n",
    "                        is_default_image = 1\n",
    "                        break\n",
    "            post_link = entry.link\n",
    "            published = datetime.now(timezone.utc).isoformat()\n",
    "            if 'published' in entry:\n",
    "                published = entry.published\n",
    "            datevalidation = is_valid_timezone_format(published)\n",
    "            if datevalidation[0]:\n",
    "                hasura_timestamp = datevalidation[1]\n",
    "            if check_date_format(published):\n",
    "                hasura_timestamp = published\n",
    "            else:\n",
    "                hasura_timestamp = datetime.now().astimezone(timezone.utc).isoformat()\n",
    "            if \"author\" in entry:\n",
    "                author = entry.author\n",
    "            else:\n",
    "                author = \"na\"\n",
    "            if outlet[i] in post_link:\n",
    "                articles.append({\n",
    "                        \"rss1_link\": rss1_links_array[i],\n",
    "                        \"post_link\": post_link,\n",
    "                        \"title\": title,\n",
    "                        \"summary\": summary,\n",
    "                        \"author\": author,\n",
    "                        \"image_link\" : image_url,\n",
    "                        \"post_published\": hasura_timestamp,\n",
    "                        \"is_default_image\": is_default_image,\n",
    "                    }\n",
    "                )\n",
    "            #print(feed_link, post_link, title, summary, author, image_url, hasura_timestamp, is_default_image)\n",
    "        mutation_variables = {\n",
    "            \"objects\": articles\n",
    "        }\n",
    "        #print({'query': mutation_query, 'variables': mutation_variables})\n",
    "        out1 = mutation_hasura_graphql(endpoint = endpoint, admin_key = admin_key, mutation_query = mutation_query, mutation_variables = mutation_variables)\n",
    "\n",
    "def update_article_details(offset1):\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    articles_T_v1_rss1_articals(limit: $limit, offset: $offset, where: {is_in_detail: {_eq: 0}}, order_by: {post_published: desc}) {\n",
    "        post_link\n",
    "        is_default_image\n",
    "        image_link\n",
    "        id\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_T_v1_rss1_articles_detail_insert_input!] = {}, $updates: [articles_T_v1_rss1_articals_updates!] = {where: {}}) {\n",
    "        insert_articles_T_v1_rss1_articles_detail(objects: $objects, on_conflict: {constraint: T_v1_rss1_articles_detail_article_id_key}) {\n",
    "            affected_rows\n",
    "        }\n",
    "        update_articles_T_v1_rss1_articals_many(updates: $updates) {\n",
    "            affected_rows\n",
    "        }\n",
    "        }\n",
    "\n",
    "    \"\"\"    \n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.use_chromium = True\n",
    "    options.page_load_strategy = 'eager'\n",
    "    options.add_argument('--enable-immersive-reader')\n",
    "    driver = webdriver.Edge(options=options)\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 2,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        #print(variables, response_data)\n",
    "        #print(response_data)\n",
    "        post_links_array = []\n",
    "        ids=[]\n",
    "        if response_data:\n",
    "            post_links_array = [item[\"post_link\"] for item in response_data[\"data\"][\"articles_T_v1_rss1_articals\"]]\n",
    "            is_default_image_array = [item[\"is_default_image\"] for item in response_data[\"data\"][\"articles_T_v1_rss1_articals\"]]\n",
    "            image_link_array = [item[\"image_link\"] for item in response_data[\"data\"][\"articles_T_v1_rss1_articals\"]]\n",
    "            ids=[item[\"id\"] for item in response_data[\"data\"][\"articles_T_v1_rss1_articals\"]]\n",
    "        articles_detail = []\n",
    "        articles_update = []\n",
    "        if len(post_links_array) == 0:\n",
    "            break\n",
    "        try:\n",
    "            for a in range(len(post_links_array)):\n",
    "                main_link = post_links_array[a]\n",
    "                print(main_link)\n",
    "                driver.get(main_link)\n",
    "                get_url = driver.current_url\n",
    "                read_link= \"read://\"+get_url\n",
    "                driver.get(read_link)\n",
    "                time.sleep(5)\n",
    "                ActionChains(driver).key_down(Keys.CONTROL).send_keys('a').key_up(Keys.CONTROL).perform()\n",
    "                ActionChains(driver).key_down(Keys.CONTROL).send_keys('c').key_up(Keys.CONTROL).perform()\n",
    "                text = pyperclip.paste()\n",
    "                text2 = text\n",
    "                text3 = text2.split('\\n')\n",
    "                text3 = [s.replace('\\r', '') for s in text3]\n",
    "                special_chars = set(\"!@#$%^&*()_+[]{}|;:'\\\",<>?\")\n",
    "                text4 = [s for s in text3 if len(s) > 0 and (s[0] not in special_chars or s[-1] not in special_chars)]\n",
    "                my_list = text4\n",
    "                if my_list[0] == \"Hmmmâ€¦ can't reach this page\":\n",
    "                    offset = offset + 1\n",
    "                    break\n",
    "                my_set = set()\n",
    "                desription = []\n",
    "                for item in my_list:\n",
    "                    if item not in my_set:\n",
    "                        desription.append(item)\n",
    "                        my_set.add(item)\n",
    "                #print(desription)\n",
    "                images_final = []\n",
    "                articles_detail.append({\n",
    "                    \"article_id\": ids[a],\n",
    "                    \"title\": desription[0],\n",
    "                    \"description\": desription[1:],\n",
    "                    \"image_link\": images_final,\n",
    "                }\n",
    "                )\n",
    "                if (is_default_image_array[a] == 0 and len(images_final) > 0):\n",
    "                    articles_update.append({\n",
    "                        \"where\": {\"post_link\" : { \"_eq\": main_link }},\n",
    "                        \"_set\": {\"is_in_detail\": 1 , \"image_link\": images_final[0], \"is_default_image\": 1}\n",
    "                    })\n",
    "                else:\n",
    "                    articles_update.append({\n",
    "                        \"where\": {\"post_link\" : { \"_eq\": main_link }},\n",
    "                        \"_set\": {\"is_in_detail\": 1}\n",
    "                    })\n",
    "                \n",
    "                #print(main_link, desription[0], desription[1:], images_final)\n",
    "            #print(articles_update)\n",
    "            mutation_variables = {\n",
    "            \"objects\": articles_detail,\n",
    "            \"updates\": articles_update,\n",
    "            }\n",
    "            out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "        except:\n",
    "            offset = offset + 1\n",
    "            mutation_variables = {\n",
    "            \"objects\": articles_detail,\n",
    "            \"updates\": articles_update,\n",
    "            }\n",
    "            out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "        \n",
    "    driver.quit() \n",
    "\n",
    "def summerizer(offset1): \n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    articles_T_v1_rss1_articles_detail(limit: $limit, offset: $offset, where: {summary: {_is_null: true}}) {\n",
    "        title\n",
    "        description\n",
    "        article_id\n",
    "        T_v1_rss1_artical {\n",
    "        title\n",
    "        summary\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($updates: [articles_T_v1_rss1_articles_detail_updates!] = {where: {}}) {\n",
    "    update_articles_T_v1_rss1_articles_detail_many(updates: $updates) {\n",
    "        affected_rows\n",
    "        returning {\n",
    "        id\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 2,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        rss1_articles_detail_updates = []\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_T_v1_rss1_articles_detail']) == 0:\n",
    "            break\n",
    "        for response in response_data['data']['articles_T_v1_rss1_articles_detail']:\n",
    "            print(response['title'])\n",
    "            article=\"\"\n",
    "            article = article + response['title'] + \" \" +  response['T_v1_rss1_artical']['title'] + \" \" +  response['T_v1_rss1_artical']['summary'] + ', '.join(response['description'])\n",
    "            chunks=[]\n",
    "            max_length = 0\n",
    "            min_length = 0\n",
    "            if len(article) < 1000:\n",
    "                max_length = 150\n",
    "                min_length = 100\n",
    "                chunks.append(article)\n",
    "            elif len(article) < 3000:\n",
    "                max_length = 300\n",
    "                min_length = 200\n",
    "                chunks.append(article)\n",
    "            elif len(article) < 4000:\n",
    "                max_length = 400\n",
    "                min_length = 250\n",
    "                chunks.append(article)\n",
    "            elif len(article) < 8000:\n",
    "                max_length = 200\n",
    "                min_length = 150\n",
    "                midpoint = len(article) // 2\n",
    "                chunks.append(article[:midpoint])\n",
    "                chunks.append(article[midpoint:])\n",
    "            else:\n",
    "                article=article[:8000]\n",
    "                max_length = 200\n",
    "                min_length = 150\n",
    "                midpoint = len(article) // 2\n",
    "                chunks.append(article[:midpoint])\n",
    "                chunks.append(article[midpoint:])\n",
    "\n",
    "            summerize=\"\"\n",
    "            for chunk in chunks:\n",
    "                summerize=summerize + summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']+ \" \"\n",
    "            if len(summerize) > 0:\n",
    "                rss1_articles_detail_updates.append({\n",
    "                    \"where\": {\"article_id\" : { \"_eq\": response['article_id'] }},\n",
    "                    \"_set\": {\"summary\": summerize }\n",
    "                })\n",
    "        mutation_variables = {\n",
    "            \"updates\": rss1_articles_detail_updates,\n",
    "            }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def vectorize(offset1):\n",
    "    model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    articles_T_v1_rss1_articals(limit: $limit, offset: $offset, where: {is_vectorized: {_eq: 0}, is_in_detail: {_eq: 1}}) {\n",
    "        id\n",
    "        title\n",
    "        summary\n",
    "        T_v1_rss1_articles_detail {\n",
    "        summary\n",
    "        tags\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_t_v1_rss1_article_vectors_insert_input!] = {}, $updates: [articles_T_v1_rss1_articals_updates!] = {where: {}}) {\n",
    "    insert_articles_t_v1_rss1_article_vectors(objects: $objects, on_conflict: {constraint: t_v1_rss1_article_vectors_article_id_key}) {\n",
    "        affected_rows\n",
    "        returning {\n",
    "        article_id\n",
    "        }\n",
    "    }\n",
    "    update_articles_T_v1_rss1_articals_many(updates: $updates) {\n",
    "        affected_rows\n",
    "        returning {\n",
    "        id\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        articles_vector1_insert_input_loc=[]\n",
    "        rss1_articals_updates_loc=[]\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_T_v1_rss1_articals']) == 0:\n",
    "            break\n",
    "        #print(json.dumps(response_data, indent=4))\n",
    "        s1= []\n",
    "        ids=[]\n",
    "        for response in response_data['data']['articles_T_v1_rss1_articals']:\n",
    "            article=\"\"\n",
    "            tags=\"\"\n",
    "            if (response['T_v1_rss1_articles_detail']['tags']) is None:\n",
    "                tags = \" \"\n",
    "            else:\n",
    "                tags = \", \".join(response['T_v1_rss1_articles_detail']['tags'])\n",
    "            article = article + response['title'] + \" \" +  response['summary'] + \" \" +  response['T_v1_rss1_articles_detail']['summary'] + tags\n",
    "            s1.append([['Represent the news article for custering and retrieval:  ', article]])\n",
    "            ids.append(response['id'])\n",
    "        embeddings = []\n",
    "        for s in s1:\n",
    "            list_embeddings = numpy.ravel(model.encode(s)).tolist()\n",
    "            embeddings.append(list_embeddings)\n",
    "        for i in range(0,len(ids)):\n",
    "            articles_vector1_insert_input_loc.append({\n",
    "                \"article_id\": ids[i],\n",
    "                \"vector1\": str(embeddings[i]),\n",
    "                }\n",
    "                )\n",
    "            rss1_articals_updates_loc.append({\n",
    "                \"where\": {\"id\" : { \"_eq\": ids[i] }},\n",
    "                \"_set\": {\"is_vectorized\": 1}\n",
    "                })\n",
    "\n",
    "        mutation_variables = {\n",
    "        \"objects\": articles_vector1_insert_input_loc,\n",
    "        \"updates\": rss1_articals_updates_loc,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def grouping(offset1):\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "        articles_T_v1_rss1_articals(limit: $limit, offset: $offset, where: {is_vectorized: {_eq: 1}, is_in_detail: {_eq: 1}, is_grouped: {_eq: 0}}) {\n",
    "            id\n",
    "            t_v1_rss1_article_vector {\n",
    "            vector1\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_t_v1_articles_groups_insert_input!] = {}, $updates: [articles_T_v1_rss1_articals_updates!] = {where: {}}) {\n",
    "        insert_articles_t_v1_articles_groups(objects: $objects, on_conflict: {constraint: t_v1_articles_groups_article_id_key}) {\n",
    "            affected_rows\n",
    "            returning {\n",
    "            article_id\n",
    "            }\n",
    "        }\n",
    "        update_articles_T_v1_rss1_articals_many(updates: $updates) {\n",
    "            affected_rows\n",
    "            returning {\n",
    "            id\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    \"\"\"\n",
    "    func_query = '''\n",
    "    query MyQuery($p_article_id: bigint!) {\n",
    "        articles_get_similar_articles_group(args: {p_article_id: $p_article_id}) {\n",
    "            article_id\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 20,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        articles_groups_insert_input_loc=[]\n",
    "        rss1_articals_updates_loc=[]\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_T_v1_rss1_articals']) == 0:\n",
    "            break\n",
    "        #print(json.dumps(response_data, indent=4))\n",
    "        s1= []\n",
    "        ids=[]\n",
    "        for response in response_data['data']['articles_T_v1_rss1_articals']:\n",
    "            func_variables = {\n",
    "                \"p_article_id\": response['id']\n",
    "                }\n",
    "            func_response_data = query_hasura_graphql(endpoint, admin_key, func_query, func_variables)\n",
    "            article_group = []\n",
    "            #print(json.dumps(func_response_data, indent=4))\n",
    "            if len(func_response_data['data']['articles_get_similar_articles_group']) > 0:\n",
    "                for func_response in func_response_data['data']['articles_get_similar_articles_group']:\n",
    "                    article_group.append(func_response['article_id'])\n",
    "            \n",
    "            articles_groups_insert_input_loc.append({\n",
    "                \"article_id\": response['id'],\n",
    "                \"initial_group\": article_group,\n",
    "                }\n",
    "                )\n",
    "            rss1_articals_updates_loc.append({\n",
    "                \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                \"_set\": {\"is_grouped\": 1}\n",
    "                })\n",
    "\n",
    "        mutation_variables = {\n",
    "        \"objects\": articles_groups_insert_input_loc,\n",
    "        \"updates\": rss1_articals_updates_loc,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def grouping_l1(offset1):\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "          articles_T_v1_rss1_articals(where: {is_grouped: {_eq: 1}}, limit: $limit, offset: $offset) {\n",
    "            id\n",
    "            t_v1_articles_group {\n",
    "            initial_group\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "        mutation MyMutation($objects: [articles_t_v1_articals_groups_l1_insert_input!] = {}, $updates: [articles_T_v1_rss1_articals_updates!] = {where: {}}, $updates1: [articles_t_v1_articals_groups_l1_updates!] = {where: {}}) {\n",
    "        insert_articles_t_v1_articals_groups_l1(objects: $objects) {\n",
    "            affected_rows\n",
    "        }\n",
    "        update_articles_T_v1_rss1_articals_many(updates: $updates) {\n",
    "            affected_rows\n",
    "        }\n",
    "        update_articles_t_v1_articals_groups_l1_many(updates: $updates1) {\n",
    "            affected_rows\n",
    "        }\n",
    "        }\n",
    "    \"\"\"\n",
    "    query2 = '''\n",
    "    query MyQuery($articleid: [bigint!] = [20]) {\n",
    "        articles_t_v1_articles_groups(where: {initial_group: {_contains: $articleid}}) {\n",
    "            article_id\n",
    "            initial_group\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    query3 = '''\n",
    "    query MyQuery($articleid: [bigint!] = [20]) {\n",
    "        articles_t_v1_articals_groups_l1(where: {articles_group: {_contains: $articleid}}) {\n",
    "            articles_group\n",
    "            id\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        articles_grouped_l1_insert_input_loc=[]\n",
    "        rss1_articals_updates_loc=[]\n",
    "        articles_grouped_l1_updates=[]\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_T_v1_rss1_articals']) == 0:\n",
    "            break\n",
    "        #print(json.dumps(response_data, indent=4))\n",
    "        for response in response_data['data']['articles_T_v1_rss1_articals']:\n",
    "\n",
    "            variables2 = {\n",
    "                \"articleid\": [response['id']]\n",
    "                }\n",
    "            func_response_data = query_hasura_graphql(endpoint, admin_key, query2, variables2)\n",
    "            articles_ids = []\n",
    "            print(response['id'])\n",
    "            if len(func_response_data['data']['articles_t_v1_articles_groups']) > 0:\n",
    "                for func_response in func_response_data['data']['articles_t_v1_articles_groups']:\n",
    "                    articles_ids.append(func_response['initial_group'])\n",
    "            \n",
    "            func_response_data1 = query_hasura_graphql(endpoint, admin_key, query3, variables2)\n",
    "            \n",
    "            \n",
    "            if (len(func_response_data1['data']['articles_t_v1_articals_groups_l1']) == 0):\n",
    "                new_lst = []\n",
    "                for sublist in articles_ids:\n",
    "                    for element in sublist:\n",
    "                        new_lst.append(element)\n",
    "                my_list = list(set(new_lst))\n",
    "                print(my_list)\n",
    "                articles_grouped_l1_insert_input_loc.append({\n",
    "                    \"articles_group\": my_list,\n",
    "                    'articles_in_group': len(my_list)\n",
    "                    }\n",
    "                    )\n",
    "                rss1_articals_updates_loc.append({\n",
    "                    \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                    \"_set\": {\"is_grouped\": 2}\n",
    "                    })\n",
    "            else:\n",
    "                articles_ids.append(func_response_data1['data']['articles_t_v1_articals_groups_l1'][0]['articles_group'])\n",
    "                new_lst = []\n",
    "                for sublist in articles_ids:\n",
    "                    for element in sublist:\n",
    "                        new_lst.append(element)\n",
    "                my_list = list(set(new_lst))\n",
    "                articles_grouped_l1_updates.append({\n",
    "                    \"where\": {\"id\" : { \"_eq\": func_response_data1['data']['articles_t_v1_articals_groups_l1'][0]['id'] }},\n",
    "                    \"_set\": {\"articles_group\": my_list, 'articles_in_group': len(my_list)}\n",
    "                    })\n",
    "                rss1_articals_updates_loc.append({\n",
    "                    \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                    \"_set\": {\"is_grouped\": 2}\n",
    "                    })\n",
    "                print(my_list)     \n",
    "        \n",
    "        mutation_variables = {\n",
    "        \"objects\": articles_grouped_l1_insert_input_loc,\n",
    "        \"updates\": rss1_articals_updates_loc,\n",
    "        \"updates1\": articles_grouped_l1_updates,\n",
    "        }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:    \n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def gen_article():\n",
    "    gpt_35_turbo_max_tokens = 4097\n",
    "    verbose = True\n",
    "    prompt_template = \"\"\"Write a unbiased professional news article for:\n",
    "\n",
    "\n",
    "        {text}\n",
    "\n",
    "\n",
    "        CONSCISE UNBIASED detailed news article with at least 500 words:\"\"\"\n",
    "    OPENAI_API_KEY= ''\n",
    "    model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model_name=model_name)\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "        articles_t_v1_articals_groups_l1(where: {requires_update: {_eq: 0}, articles_in_group: {_gt: 1}}, limit: $limit, offset: $offset) {\n",
    "            articles_group\n",
    "            id\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    graphql_grquery_article = '''\n",
    "    query MyQuery($article_id: bigint!) {\n",
    "    articles_T_v1_rss1_articals(where: {id: {_eq: $article_id}}) {\n",
    "        title\n",
    "        summary\n",
    "        T_v1_rss1_articles_detail {\n",
    "        summary\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    offset = 0\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($objects: [articles_t_v1_articals_groups_l1_detail_insert_input!] = {}, $updates: [articles_t_v1_articals_groups_l1_updates!] = {where: {}}) {\n",
    "        insert_articles_t_v1_articals_groups_l1_detail(objects: $objects, on_conflict: {constraint: t_v1_articals_groups_l1_detail_article_group_id_key}) {\n",
    "            affected_rows\n",
    "        }\n",
    "        update_articles_t_v1_articals_groups_l1_many(updates: $updates) {\n",
    "            affected_rows\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_t_v1_articals_groups_l1']) == 0:\n",
    "            break\n",
    "        for response in response_data['data']['articles_t_v1_articals_groups_l1']:\n",
    "            llm_text = ''\n",
    "            articles_t_v1_articals_groups_l1_detail_insert_input=[]\n",
    "            articles_t_v1_articals_groups_l1_updates=[]\n",
    "            for article in response['articles_group']:\n",
    "                article_variables = {\n",
    "                \"article_id\": article\n",
    "                }\n",
    "                article_response_data = query_hasura_graphql(endpoint, admin_key, graphql_grquery_article, article_variables)\n",
    "                llm_text = llm_text + \"\\n\" +article_response_data['data']['articles_T_v1_rss1_articals'][0]['title'] + \"\\n\" + article_response_data['data']['articles_T_v1_rss1_articals'][0]['summary'] + \"\\n\" + article_response_data['data']['articles_T_v1_rss1_articals'][0]['T_v1_rss1_articles_detail']['summary']\n",
    "            max_tokens = 3000\n",
    "            if len(llm_text.split()) > max_tokens:\n",
    "                llm_text = ' '.join(llm_text.split()[:max_tokens])\n",
    "            print(num_tokens_from_string(llm_text, model_name))\n",
    "            \n",
    "            texts = text_splitter.split_text(llm_text)\n",
    "            docs = [Document(page_content=t) for t in texts]\n",
    "            max_tokens = 4000\n",
    "            if len(llm_text.split()) > max_tokens:\n",
    "                llm_text = ' '.join(llm_text.split()[:max_tokens])\n",
    "            prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "            num_tokens = num_tokens_from_string(llm_text, model_name)\n",
    "            print(num_tokens)\n",
    "            if num_tokens < gpt_35_turbo_max_tokens:\n",
    "                chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=prompt)\n",
    "            else:\n",
    "                print(\"map reduce\")\n",
    "                chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt=prompt, combine_prompt=prompt, verbose=verbose)\n",
    "\n",
    "            summary = chain.run(docs)\n",
    "\n",
    "            #print(f\"Chain type: {chain.__class__.__name__}\")\n",
    "            #print(f\"Run time: {monotonic() - start_time}\")\n",
    "            #print(f\"Summary: {textwrap.fill(summary, width=100)}\")\n",
    "            articles_t_v1_articals_groups_l1_detail_insert_input.append({\n",
    "                        \"article_group_id\": response['id'],\n",
    "                        'summary': summary\n",
    "                        }\n",
    "                        )\n",
    "            articles_t_v1_articals_groups_l1_updates.append({\n",
    "                        \"where\": {\"id\" : { \"_eq\": response['id'] }},\n",
    "                        \"_set\": {\"requires_update\": 1}\n",
    "                        })\n",
    "            mutation_variables = {\n",
    "            \"objects\": articles_t_v1_articals_groups_l1_detail_insert_input,\n",
    "            \"updates\": articles_t_v1_articals_groups_l1_updates,\n",
    "            }\n",
    "            out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def gen_title():\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "            articles_t_v1_articals_groups_l1_detail(where: {title: {_is_null: true}}, limit: $limit, offset: $offset) {\n",
    "            article_group_id\n",
    "            summary\n",
    "        t_v1_articals_groups_l1 {\n",
    "        articles_group\n",
    "        }\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    offset = 0\n",
    "    mutation_query= \"\"\"\n",
    "    mutation MyMutation($updates: [articles_t_v1_articals_groups_l1_detail_updates!] = {where: {}}) {\n",
    "  update_articles_t_v1_articals_groups_l1_detail_many(updates: $updates) {\n",
    "    affected_rows\n",
    "  }\n",
    "}\n",
    "\n",
    "    \"\"\"\n",
    "    variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "    while True:\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_t_v1_articals_groups_l1_detail']) == 0:\n",
    "            break\n",
    "        update_articles_t_v1_articals_groups_l1_detail_many_loc=[]\n",
    "        for response in response_data['data']['articles_t_v1_articals_groups_l1_detail']:\n",
    "            input_text = \"generate a intresting and viral news title for:  \" + \"\\n\" + response['summary']\n",
    "            max_length = 512\n",
    "            input_text = input_text[:max_length]\n",
    "            input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "            outputs = model.generate(input_ids, max_new_tokens=50)\n",
    "            generated_text = tokenizer.decode(outputs[0])\n",
    "            clean_text = re.sub('<.*?>', '', generated_text) # remove data between < and >\n",
    "            print(clean_text)\n",
    "            update_articles_t_v1_articals_groups_l1_detail_many_loc.append({\n",
    "                \"where\": {\"article_group_id\" : { \"_eq\": response['article_group_id'] }},\n",
    "                \"_set\": {\"title\": clean_text}\n",
    "                })\n",
    "        mutation_variables = {\n",
    "            \"updates\": update_articles_t_v1_articals_groups_l1_detail_many_loc,\n",
    "            }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def gen_images():\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "        articles_t_v1_articals_groups_l1_detail(where: {image_urls: {_is_null: true}, _and: {logo_urls: {_is_null: true}}}, limit: $limit, offset: $offset) {\n",
    "            article_group_id\n",
    "            id\n",
    "            t_v1_articals_groups_l1 {\n",
    "            articles_group\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    '''\n",
    "    offset = 0\n",
    "    graphql_query2 = '''\n",
    "    query MyQuery($article_id: bigint! ) {\n",
    "    articles_T_v1_rss1_articals(where: {id: {_eq: $article_id}}) {\n",
    "        image_link\n",
    "        t_v1_rss1_feed_link {\n",
    "        t_v1_outlet {\n",
    "            logo_url\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    mutation_query= \"\"\"\n",
    "    mutation MyMutation($updates: [articles_t_v1_articals_groups_l1_detail_updates!] = {where: {}}) {\n",
    "    update_articles_t_v1_articals_groups_l1_detail_many(updates: $updates) {\n",
    "    affected_rows\n",
    "    }\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    variables = {\n",
    "        \"limit\": 1,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "    while True:\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        #print(json.dumps(response_data, indent=4))\n",
    "        if len(response_data['data']['articles_t_v1_articals_groups_l1_detail']) == 0:\n",
    "            break\n",
    "        articles_t_v1_articals_groups_l1_detail_updates_loc=[]\n",
    "        for response in response_data['data']['articles_t_v1_articals_groups_l1_detail']:\n",
    "            #print(json.dumps(response, indent=4))\n",
    "            image_links=[]\n",
    "            logo_links=[]\n",
    "            for article_id in response['t_v1_articals_groups_l1']['articles_group']:\n",
    "                variables2 = {\n",
    "                    \"article_id\": article_id\n",
    "                    }\n",
    "                response_data_article = query_hasura_graphql(endpoint, admin_key, graphql_query2, variables2)\n",
    "                if response_data_article['data']['articles_T_v1_rss1_articals'][0]['image_link'] != '':\n",
    "                    image_links.append(response_data_article['data']['articles_T_v1_rss1_articals'][0]['image_link'])\n",
    "                if response_data_article['data']['articles_T_v1_rss1_articals'][0]['t_v1_rss1_feed_link']['t_v1_outlet']['logo_url'] != '':\n",
    "                    logo_links.append(response_data_article['data']['articles_T_v1_rss1_articals'][0]['t_v1_rss1_feed_link']['t_v1_outlet']['logo_url'])\n",
    "            #print(image_links)\n",
    "            #print(logo_links)\n",
    "            image_links = list(set(image_links))\n",
    "            logo_links = list(set(logo_links))\n",
    "            articles_t_v1_articals_groups_l1_detail_updates_loc.append({\n",
    "                \"where\": {\"article_group_id\" : { \"_eq\": response['article_group_id'] }},\n",
    "                \"_set\": {\"image_urls\": image_links, \"logo_urls\": logo_links}\n",
    "                })  \n",
    "        mutation_variables = {\n",
    "            \"updates\": articles_t_v1_articals_groups_l1_detail_updates_loc,\n",
    "            }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n",
    "def summerizer_60_words(offset1): \n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    graphql_query = '''\n",
    "    query MyQuery($limit: Int!, $offset: Int!) {\n",
    "    articles_t_v1_articals_groups_l1_detail(limit: $limit, offset: $offset, where: {summary_60_words: {_is_null: true}}) {\n",
    "        article_group_id\n",
    "        summary\n",
    "    }\n",
    "    }\n",
    "    '''\n",
    "    offset = offset1\n",
    "    mutation_query = \"\"\"\n",
    "    mutation MyMutation($updates: [synopse_articles_t_v4_article_groups_l2_detail_updates!] = {where: {}}) {\n",
    "        update_synopse_articles_t_v4_article_groups_l2_detail_many(updates: $updates) {\n",
    "            affected_rows\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        variables = {\n",
    "        \"limit\": 2,\n",
    "        \"offset\": offset\n",
    "        }\n",
    "        articles_t_v1_articals_groups_l1_detail_updates_loc = []\n",
    "        response_data = query_hasura_graphql(endpoint, admin_key, graphql_query, variables)\n",
    "        if len(response_data['data']['articles_t_v1_articals_groups_l1_detail']) == 0:\n",
    "            break\n",
    "        for response in response_data['data']['articles_t_v1_articals_groups_l1_detail']:\n",
    "            print(response['article_group_id'])\n",
    "            article = response['summary'] \n",
    "            max_length = 65\n",
    "            min_length = 45\n",
    "            \n",
    "            summerize= summarizer(article, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']+ \" \"\n",
    "            if len(summerize) > 0:\n",
    "                articles_t_v1_articals_groups_l1_detail_updates_loc.append({\n",
    "                    \"where\": {\"article_group_id\" : { \"_eq\": response['article_group_id'] }},\n",
    "                    \"_set\": {\"summary_60_words\": summerize }\n",
    "                })\n",
    "        mutation_variables = {\n",
    "            \"updates\": articles_t_v1_articals_groups_l1_detail_updates_loc,\n",
    "            }\n",
    "        out1 = mutation_hasura_graphql(endpoint=endpoint, admin_key=admin_key, mutation_query=mutation_query, mutation_variables=mutation_variables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rss1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
